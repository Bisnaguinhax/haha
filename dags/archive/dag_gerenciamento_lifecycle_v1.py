# -*- coding: utf-8 -*-
"""
DAG de Gerenciamento de Lifecycle (HOT -> COLD STORAGE) - DEMONSTRAÃ‡ÃƒO
"""
# ===================================================================================
# 1. CORREÃ‡ÃƒO: A importaÃ§Ã£o __future__ deve ser a primeira linha de cÃ³digo.
# ===================================================================================
from __future__ import annotations

import os
from datetime import datetime, timedelta

import pendulum
from botocore.exceptions import ClientError

from airflow.models.dag import DAG
from airflow.decorators import task

# ===================================================================================
# 2. CORREÃ‡ÃƒO: A lÃ³gica para criar o cliente MinIO foi reestruturada para
# funcionar corretamente e ser reutilizÃ¡vel pelas tarefas.
# ===================================================================================
def _get_minio_client():
    """
    Cria e retorna um cliente boto3 para interagir com o MinIO.

    As credenciais sÃ£o obtidas de forma segura atravÃ©s do AirflowSecurityManager,
    que lÃª os segredos do Vault. A funÃ§Ã£o garante que as variÃ¡veis de ambiente
    necessÃ¡rias estejam presentes antes de tentar a conexÃ£o.
    """
    # Importa o SecurityManager de dentro do diretÃ³rio de plugins
    from plugins.security_system.vault import AirflowSecurityManager
    
    # ObtÃ©m a chave secreta para decriptar o Vault
    secret_key = os.getenv('SECURITY_VAULT_SECRET_KEY')
    if not secret_key:
        raise ValueError("ERRO CRÃTICO: A variÃ¡vel de ambiente 'SECURITY_VAULT_SECRET_KEY' nÃ£o estÃ¡ definida no ambiente Airflow.")

    # 3. CORREÃ‡ÃƒO: O caminho do Vault foi corrigido para o ambiente Docker.
    # O ideal Ã© que o prÃ³prio SecurityManager saiba seu caminho, mas para este
    # exemplo, usamos o caminho correto dentro do contÃªiner.
    vault_path = '/opt/airflow/plugins/security_system/vault.json'
    
    # O logger aqui Ã© simplificado pois este helper Ã© chamado por tasks,
    # que jÃ¡ tÃªm seu prÃ³prio contexto de log.
    class SimpleLogger:
        def log(self, *args, **kwargs): pass
        
    sec_manager = AirflowSecurityManager(vault_path=vault_path, secret_key=secret_key, logger=SimpleLogger())
    minio_creds = sec_manager.get_secret("minio_local_credentials")
    
    if not minio_creds:
        raise ValueError("Credenciais do MinIO 'minio_local_credentials' nÃ£o encontradas no Vault.")
    
    # Retorna o cliente S3 (boto3) configurado
    return boto3.client(
        "s3",
        endpoint_url=minio_creds['endpoint_url'],
        aws_access_key_id=minio_creds['access_key'],
        aws_secret_access_key=minio_creds['secret_key'],
    )


@task
def criar_bucket_cold_storage_task():
    """Garante que o bucket de cold storage (destino) exista antes da movimentaÃ§Ã£o."""
    s3 = _get_minio_client()
    bucket_destino = "cold-storage-layer"
    try:
        s3.head_bucket(Bucket=bucket_destino)
        print(f"âœ”ï¸ Bucket de cold storage '{bucket_destino}' jÃ¡ existe.")
    except ClientError as e:
        # Se o erro for 'Not Found', o bucket nÃ£o existe e podemos criÃ¡-lo.
        if e.response['Error']['Code'] == '404':
            print(f"Bucket '{bucket_destino}' nÃ£o encontrado. Criando...")
            s3.create_bucket(Bucket=bucket_destino)
            print(f"ðŸ§Š Bucket de cold storage '{bucket_destino}' criado com sucesso.")
        else:
            # Propaga outros erros (ex: permissÃ£o negada)
            print(f"âŒ Erro inesperado ao checar o bucket: {e}")
            raise

@task
def mover_arquivos_antigos_task():
    """Verifica e move arquivos antigos do bucket 'hot' para o 'cold' storage."""
    s3 = _get_minio_client()
    bucket_origem = "bronze-layer"
    bucket_destino = "cold-storage-layer"
    limite_dias = 30
    hoje = datetime.now()

    print(f"ðŸ”Ž Verificando arquivos no bucket '{bucket_origem}' com mais de {limite_dias} dias...")
    
    try:
        paginator = s3.get_paginator('list_objects_v2')
        pages = paginator.paginate(Bucket=bucket_origem)
        arquivos_movidos = 0

        for page in pages:
            if 'Contents' not in page:
                continue

            for objeto in page['Contents']:
                data_modificacao = objeto['LastModified'].replace(tzinfo=None)
                nome_ficheiro = objeto['Key']
                
                if (hoje - data_modificacao) > timedelta(days=limite_dias):
                    print(f"   -> ðŸ¥¶ Arquivo '{nome_ficheiro}' Ã© antigo. Movendo para cold storage...")
                    try:
                        copy_source = {'Bucket': bucket_origem, 'Key': nome_ficheiro}
                        s3.copy_object(Bucket=bucket_destino, CopySource=copy_source, Key=nome_ficheiro)
                        s3.delete_object(Bucket=bucket_origem, Key=nome_ficheiro)
                        print(f"      -> âœ… Movido e removido da origem com sucesso.")
                        arquivos_movidos += 1
                    except Exception as e:
                        print(f"      -> âŒ Erro ao mover o arquivo '{nome_ficheiro}': {e}")
                else:
                    print(f"   -> ðŸ”¥ Arquivo '{nome_ficheiro}' Ã© recente. Nenhuma aÃ§Ã£o necessÃ¡ria.")
        
        if arquivos_movidos == 0:
            print("âœ… Nenhum arquivo antigo encontrado para mover.")

    except ClientError as e:
        if e.response['Error']['Code'] == 'NoSuchBucket':
            print(f"âš ï¸ O bucket de origem '{bucket_origem}' nÃ£o existe. Nada a fazer.")
        else:
            print(f"âŒ Erro inesperado ao listar arquivos: {e}")
            raise


with DAG(
    dag_id='dag_gerenciamento_lifecycle_v1',
    start_date=pendulum.datetime(2025, 6, 10, tz="UTC"),
    schedule_interval='@daily',
    catchup=False,
    doc_md="### Gerenciamento de Lifecycle\nSimula polÃ­tica de arquivamento movendo dados antigos da camada 'hot' para 'cold'.",
    tags=['lifecycle', 'minio', 'storage', 'LGC'],
) as dag:

    criar_bucket_task = criar_bucket_cold_storage_task()
    mover_arquivos_task = mover_arquivos_antigos_task()

    criar_bucket_task >> mover_arquivos_task
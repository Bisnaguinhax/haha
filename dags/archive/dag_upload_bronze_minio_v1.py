# ===================================================================================
# DAG DE UPLOAD PARA DATA LAKE (CAMADA BRONZE) - DEMONSTRA√á√ÉO T√âCNICA
# ===================================================================================
# Esta DAG orquestra o envio de arquivos locais para o MinIO (Data Lake),
# utilizando um sistema seguro baseado em Vault e logging de auditoria.
#
# üîê SEGURAN√áA:
# - As credenciais do MinIO s√£o extra√≠das do Vault via objeto seguro.
# - Caminhos de arquivos s√£o relativos ao ambiente do cont√™iner.
# - Os uploads s√£o rastreados por sistema de auditoria customizado.
#
# ‚úÖ OBJETIVO:
# Popular a camada Bronze com dados brutos prontos para ingest√£o futura.
#
# üìå INSTRU√á√ïES:
# 1. Execute `setup_vault_secrets.py` para registrar as credenciais no Vault.
# 2. Depois, rode esta DAG manualmente para validar os uploads.
# ===================================================================================

from __future__ import annotations
import os
import pendulum
from pathlib import Path

# Importa√ß√µes podem falhar se os pacotes n√£o estiverem instalados,
# o ideal √© tratar isso no Dockerfile com um requirements.txt
try:
    import boto3
    from botocore.exceptions import ClientError
except ImportError:
    boto3 = None
    ClientError = None

from airflow.models.dag import DAG
from airflow.operators.python import PythonOperator

# Caminhos relativos ao ambiente do cont√™iner Airflow
# Estes caminhos correspondem aos volumes montados no docker-compose.yml
AIRFLOW_HOME = os.getenv('AIRFLOW_HOME', '/opt/airflow')
AUDIT_LOG_PATH = f'{AIRFLOW_HOME}/logs/security_audit/audit.csv'
SYSTEM_LOG_PATH = f'{AIRFLOW_HOME}/logs/security_audit/system.log'
VAULT_DB_PATH = f'{AIRFLOW_HOME}/data/security_vault.db'


# === üöÄ Fun√ß√£o principal de upload ===
def _upload_para_minio_seguro():
    """
    Recupera credenciais do MinIO via Vault e realiza o upload
    de arquivos locais para a camada Bronze do Data Lake.
    """
    if boto3 is None:
        raise RuntimeError("A biblioteca 'boto3' n√£o est√° instalada. Adicione 'boto3' ao requirements.txt.")

    # Importa√ß√£o local (isolada) do sistema de seguran√ßa
    from plugins.security_system.audit import AuditLogger
    from plugins.security_system.vault import AirflowSecurityManager

    SECRET_KEY = os.getenv('SECURITY_VAULT_SECRET_KEY')
    if not SECRET_KEY:
        raise ValueError("ERRO CR√çTICO: A vari√°vel de ambiente 'SECURITY_VAULT_SECRET_KEY' n√£o est√° definida.")

    print("üîê Obtendo credenciais seguras do Vault...")
    
    # Inicializa os componentes de seguran√ßa
    audit = AuditLogger(AUDIT_LOG_PATH, SYSTEM_LOG_PATH)
    sec_manager = AirflowSecurityManager(VAULT_DB_PATH, SECRET_KEY, audit)

    minio_creds = sec_manager.get_secret("minio_local_credentials")
    if not minio_creds:
        raise ValueError("‚ùå ERRO: Credenciais 'minio_local_credentials' n√£o encontradas no Vault.")

    bucket_name = "bronze" # Camada bronze geralmente tem nome simples
    print(f"üåê Conectando ao MinIO em: {minio_creds['endpoint_url']}")

    # Cliente MinIO
    s3_client = boto3.client(
        "s3",
        endpoint_url=minio_creds['endpoint_url'],
        aws_access_key_id=minio_creds['access_key'],
        aws_secret_access_key=minio_creds['secret_key'],
    )

    # Verifica ou cria o bucket
    try:
        s3_client.head_bucket(Bucket=bucket_name)
        print(f"‚úîÔ∏è Bucket '{bucket_name}' j√° existe.")
    except ClientError:
        s3_client.create_bucket(Bucket=bucket_name)
        print(f"ü™£ Bucket '{bucket_name}' criado com sucesso.")

    # Mapeia arquivos locais (dentro do cont√™iner) -> objetos no MinIO
    # O caminho base agora √© o do cont√™iner
    base_path_container = f'{AIRFLOW_HOME}/data'
    arquivos_para_upload = {
        f"{base_path_container}/clima/clima_coletado.csv": "clima/clima_coletado.csv",
        f"{base_path_container}/indicadores/ipca_coletado.csv": "indicadores/ipca_coletado.csv",
        f"{base_path_container}/olist/dados_consolidados.csv": "olist/dados_consolidados.csv"
    }

    print("\nüì§ Iniciando upload dos arquivos...")

    for caminho_local, caminho_minio in arquivos_para_upload.items():
        caminho = Path(caminho_local)
        if caminho.exists():
            print(f"   ‚¨ÜÔ∏è {caminho.name} ‚Üí s3://{bucket_name}/{caminho_minio}")
            s3_client.upload_file(str(caminho), bucket_name, caminho_minio)
            # Log de auditoria
            audit.log('MINIO_UPLOAD_SUCCESS', f'Arquivo {caminho.name} enviado para o bucket {bucket_name}.', user='airflow_dag')
        else:
            print(f"   ‚ö†Ô∏è Arquivo n√£o encontrado, pulando: {caminho_local}")
            audit.log('FILE_NOT_FOUND', f'Arquivo de origem n√£o encontrado em {caminho_local}.', user='airflow_dag')

    print("\n‚úÖ Upload conclu√≠do com sucesso para o MinIO Bronze.")


# === üìÖ Defini√ß√£o da DAG ===
with DAG(
    dag_id="dag_upload_bronze_minio_v1",
    start_date=pendulum.datetime(2025, 6, 10, tz="UTC"),
    schedule=None,
    catchup=False,
    doc_md="""
### DAG de Upload para a Camada Bronze
Esta DAG realiza o envio de arquivos brutos para o MinIO com seguran√ßa.
As credenciais s√£o recuperadas de um Vault local e todo o processo √© auditado.
""",
    tags=['bronze', 'minio', 'upload', 'vault']
) as dag:

    tarefa_upload_minio = PythonOperator(
        task_id="upload_ficheiros_para_camada_bronze",
        python_callable=_upload_para_minio_seguro
    )
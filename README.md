# üöÄ Pipeline de Dados Seguro: Da Ingest√£o √† Visualiza√ß√£o Anal√≠tica

<p align="center">
  <img src="docs/images/pipeline_cover.png" alt="Capa do Projeto" width="700"/>
</p>

<p align="center">
  <em>Uma arquitetura de engenharia de dados robusta, segura e 100% reprodut√≠vel para ambientes empresariais.</em>
</p>

---

## üìã √çndice

* [I. üéØ Objetivo do Case](#i--objetivo-do-case)
* [II. üèõÔ∏è Arquitetura da Solu√ß√£o](#ii--arquitetura-da-solu√ß√£o)
* [III. ‚öôÔ∏è O Case em A√ß√£o](#iii--o-case-em-a√ß√£o)
* [IV. üß† Melhorias e Pr√≥ximos Passos](#iv--melhorias-e-pr√≥ximos-passos)
* [V. üõ†Ô∏è Reprodutibilidade da Arquitetura](#v--reprodutibilidade-da-arquitetura)
* [VI. üìä Resultados e Evid√™ncias](#vi--resultados-e-evid√™ncias)

---

## I. üéØ Objetivo do Case

### Desafio
Construir um **pipeline de dados ponta a ponta** em uma arquitetura **100% local e open-source**, garantindo total reprodutibilidade. A solu√ß√£o abrange desde a ingest√£o de m√∫ltiplas fontes at√© a cria√ß√£o de um dashboard anal√≠tico interativo, com um foco rigoroso em **seguran√ßa, qualidade, governan√ßa e automa√ß√£o**.

### Compet√™ncias Demonstradas
Este projeto √© uma evid√™ncia pr√°tica de **compet√™ncias avan√ßadas em Engenharia de Dados**, abrangendo:

- **üîß Orquestra√ß√£o de fluxos complexos e resilientes** com Apache Airflow.
- **‚ö° Processamento de dados em larga escala** com Apache Spark.
- **üèóÔ∏è Modelagem dimensional e arquitetura Star Schema** para Data Warehouses.
- **üîê Desenvolvimento de um Framework de Seguran√ßa customizado**, com Vault de segredos e auditoria.
- **üìä Implementa√ß√£o de Quality Gates** com Great Expectations.
- **üèõÔ∏è Constru√ß√£o de uma arquitetura de Data Lake Medallion** (Bronze, Silver, Gold) com MinIO.
- **ü§ñ Automa√ß√£o de processos** de setup e refatora√ß√£o de c√≥digo.
- **üìà Visualiza√ß√£o anal√≠tica e Business Intelligence** com Streamlit e Grafana.
- **üê≥ Gest√£o de ambientes com Docker e Docker Compose** para portabilidade total.

### Valor de Neg√≥cio
A solu√ß√£o demonstra uma abordagem de engenharia completa, preparada para os desafios de ambientes de produ√ß√£o. Ela possibilita o processamento seguro de dados heterog√™neos, a gera√ß√£o de insights acion√°veis e serve como um **template robusto para pipelines futuros com foco em compliance e governan√ßa**.

---

## II. üèõÔ∏è Arquitetura da Solu√ß√£o

### Vis√£o Geral da Arquitetura

> **Nota Importante:** O GitHub n√£o renderiza diagramas Mermaid diretamente. A melhor pr√°tica √© gerar uma imagem (PNG/SVG) a partir do c√≥digo Mermaid e inseri-la aqui. Voc√™ pode usar o [Editor Online do Mermaid](https://mermaid.live) para gerar a imagem.

<p align="center">
  <img src="docs/images/architecture_diagram.png" alt="Vis√£o Geral da Arquitetura" width="800"/>
  <br>
  <em><strong>A√ß√£o Necess√°ria:</strong> Gere a imagem do seu diagrama Mermaid e salve em <code>docs/images/architecture_diagram.png</code> (ou atualize o caminho).</em>
</p>


### Detalhamento dos Componentes

- **Apache Airflow:** O c√©rebro da orquestra√ß√£o, gerenciando o agendamento e a execu√ß√£o de todos os pipelines (DAGs).
- **PostgreSQL:** Atua como o banco de metadados do Airflow e como o Data Warehouse final, hospedando o modelo Star Schema.
- **MinIO:** Simula um Object Storage (compat√≠vel com S3) para nosso Data Lake, com as camadas Bronze, Silver, Gold e Cold.
- **Redis:** Broker de mensagens para o Celery Executor do Airflow, permitindo a execu√ß√£o distribu√≠da de tarefas.
- **Apache Spark:** Motor de processamento distribu√≠do para transforma√ß√µes e agrega√ß√µes em larga escala.
- **Streamlit:** Interface para construir dashboards interativos de Business Intelligence.
- **Grafana:** Ferramenta para monitoramento e visualiza√ß√£o de m√©tricas operacionais.

### üîê Framework de Seguran√ßa Customizado (`plugins/security_system/`)

> **Diferencial do Projeto:** Este framework garante a integridade, confidencialidade e rastreabilidade dos dados em todo o pipeline, demonstrando conhecimento avan√ßado em seguran√ßa e engenharia de software.

- **Security Vault:** Um cofre digital criptografado (AES-128) que armazena credenciais sens√≠veis e as fornece em tempo de execu√ß√£o, eliminando o *hardcoding*.
- **Audit Logger:** Um sistema de auditoria que registra todas as opera√ß√µes cr√≠ticas (acessos a segredos, transforma√ß√µes, valida√ß√µes), essencial para conformidade (LGPD, SOX).
- **Secure Connection Pool:** Um gerenciador que facilita a obten√ß√£o segura de conex√µes a servi√ßos externos (MinIO, PostgreSQL), abstraindo o uso do Vault.

### üóÑÔ∏è Data Lake com Arquitetura Medallion (MinIO)

| Camada         | Descri√ß√£o                    | Caracter√≠sticas                                              |
| :------------- | :--------------------------- | :----------------------------------------------------------- |
| ü•â **Bronze** | Dados Brutos e Imut√°veis     | Raw data, schema-on-read, audit√°vel.                         |
| ü•à **Silver** | Dados Limpos e Padronizados  | LGPD compliant (PII mascarado), validado, pronto para an√°lise. |
| ü•á **Gold** | Dados Agregados e Otimizados | Regras de neg√≥cio aplicadas, sumarizado, alta performance para BI. |
| üßä **Cold Storage** | Dados Arquivados/Inativos    | Otimiza√ß√£o de custos, reten√ß√£o de longo prazo.               |

### ‚ö° Processamento Distribu√≠do (Apache Spark)
Utilizado para transforma√ß√µes complexas, executando limpeza, normaliza√ß√£o, enriquecimento e agrega√ß√£o, com credenciais injetadas de forma segura em tempo de execu√ß√£o e persistindo os dados no formato colunar otimizado Parquet.

### üìä Qualidade de Dados (Great Expectations)
Implementamos **Quality Gates** em etapas cr√≠ticas. Os pipelines s√£o interrompidos automaticamente se as expectativas de qualidade n√£o forem atendidas, prevenindo a propaga√ß√£o de dados ruins e garantindo a confian√ßa nos resultados.

---

## III. ‚öôÔ∏è O Case em A√ß√£o

### Fluxo de Trabalho do Pipeline

<p align="center">
  <img src="docs/images/pipeline_flow.png" alt="Fluxo de Trabalho do Pipeline" width="800"/>
  <br>
  <em><strong>A√ß√£o Necess√°ria:</strong> Gere a imagem do seu segundo diagrama e salve em <code>docs/images/pipeline_flow.png</code> (ou atualize o caminho).</em>
</p>

### üîÑ Etapas Detalhadas do Pipeline

1.  **Coleta Segura:**
    - **Objetivo:** Ingest√£o de dados brutos de APIs e datasets.
    - **Seguran√ßa:** Credenciais s√£o obtidas do Security Vault.
    - **Destino:** Camada **Bronze** do MinIO.

2.  **Consolida√ß√£o e Mascaramento PII:**
    - **Objetivo:** Limpar, unificar e proteger dados sens√≠veis (LGPD).
    - **Destino:** Camada **Silver** do MinIO.

3.  **Processamento em Larga Escala com Spark:**
    - **Objetivo:** Transformar dados da camada Silver em agregados para BI.
    - **Seguran√ßa:** Credenciais para o MinIO s√£o injetadas de forma segura no Spark.
    - **Destino:** Camada **Gold** do MinIO.

4.  **Valida√ß√£o de Qualidade:**
    - **Objetivo:** Assegurar a integridade dos dados com Great Expectations, atuando como um "Quality Gate".
    - **Auditoria:** Resultados das valida√ß√µes s√£o registrados no Audit Logger.

5.  **Carga no Data Mart (Star Schema):**
    - **Objetivo:** Carregar dados da camada Gold para o PostgreSQL de forma transacional (ACID).
    - **Seguran√ßa:** Conex√£o segura ao PostgreSQL utilizando credenciais do Vault.

6.  **Gerenciamento de Lifecycle:**
    - **Objetivo:** Otimizar custos movendo dados antigos da camada Bronze para o Cold Storage.

### Fontes de Dados Integradas

| Fonte           | Tipo       | Descri√ß√£o                        | Volume Simulado |
| :-------------- | :--------- | :------------------------------- | :-------------- |
| **Banco Central** | API REST   | Indicadores econ√¥micos (IPCA)    | Pequeno         |
| **OpenWeather** | API REST   | Dados meteorol√≥gicos por regi√£o  | Pequeno         |
| **Olist** | Dataset CSV | Dados reais de e-commerce brasileiro | Grande          |

---

## IV. üß† Melhorias e Pr√≥ximos Passos

### üöÄ Melhorias Propostas
- **üèóÔ∏è Infraestrutura como C√≥digo (IaC):** Utilizar **Terraform** e **Ansible**.
- **üîÑ CI/CD para Pipelines:** Implementar **GitHub Actions / GitLab CI**.
- **üìö Cat√°logo de Dados:** Integrar com **Apache Atlas** ou **Amundsen**.
- **üî≠ Observabilidade Avan√ßada:** Usar **Prometheus / Grafana** e **Jaeger / OpenTelemetry**.

### üìà Proje√ß√µes de Escalabilidade

| Aspecto       | Implementa√ß√£o Atual (Local) | Proposta de Melhoria (Cloud)             |
| :------------ | :-------------------------- | :--------------------------------------- |
| **Volume** | ~100k registros             | **Petabytes** (particionamento, sharding) |
| **Lat√™ncia** | < 30s (end-to-end)          | **< 10s** (ingest√£o com Kafka/Redis)      |
| **Concorr√™ncia**| 3 DAGs paralelas            | **10+ DAGs e tasks simult√¢neas** (K8s)    |
| **Monitoramento** | Logs b√°sicos, Airflow UI    | **APM completo**, dashboards, alertas    |

### üèÜ Considera√ß√µes Finais

> Este case entrega uma solu√ß√£o de dados **enterprise-grade, segura, confi√°vel e totalmente reprodut√≠vel**. As decis√µes de projeto demonstram um dom√≠nio de conceitos que v√£o muito al√©m do b√°sico, focando nos desafios reais de um ambiente corporativo. A arquitetura est√° pronta para ser adaptada e estendida para ambientes de nuvem em grande escala.

---

## V. üõ†Ô∏è Reprodutibilidade da Arquitetura

### Pr√©-requisitos e Instala√ß√£o

1.  **Pr√©-requisitos:**
    - Python 3.8+, Git, Docker e Docker Compose.
    - **Hardware M√≠nimo:** 8GB RAM (16GB recomendado), 4 CPU cores, 10GB de armazenamento.

2.  **Clonagem do Reposit√≥rio:**
    ```bash
    git clone [https://github.com/felipesbonatti/case-data-master-engenharia-de-dados.git](https://github.com/felipesbonatti/case-data-master-engenharia-de-dados.git)
    cd case-data-master-engenharia-de-dados
    ```

3.  **Configura√ß√£o do Ambiente:**
    > **Aten√ß√£o:** Este passo √© CR√çTICO para a seguran√ßa e funcionamento do pipeline.

    ```bash
    # Crie o arquivo de ambiente a partir do template
    cp .env.example .env

    # Gere uma chave de criptografia segura e adicione-a ao .env
    python -c "from cryptography.fernet import Fernet; print('SECURITY_VAULT_SECRET_KEY=' + Fernet.generate_key().decode())" >> .env

    # ABRA O ARQUIVO .env E CONFIGURE SUAS API KEYS E SENHAS REAIS!
    ```

4.  **Inicializa√ß√£o da Infraestrutura:**
    ```bash
    # Garante um ambiente limpo, reconstruindo as imagens e depend√™ncias
    docker-compose down -v --rmi all && docker system prune -a --volumes -f

    # Inicia todos os servi√ßos em background
    docker-compose up -d --build
    ```

5.  **Configura√ß√£o do Security Vault e Airflow:**
    ```bash
    # Acesse o cont√™iner do Airflow scheduler
    docker-compose exec airflow-scheduler bash

    # Dentro do cont√™iner, popule o Vault com as credenciais do seu .env
    python /opt/airflow/scripts/setup_vault_secrets.py
    
    # Saia do cont√™iner
    exit

    # Inicialize o banco de dados e crie o usu√°rio admin
    docker-compose exec airflow-webserver airflow db upgrade
    docker-compose exec airflow-webserver airflow users create \
        --username admin --password admin \
        --firstname Admin --lastname User \
        --role Admin --email admin@example.com || true
    ```

### Verifica√ß√£o e Execu√ß√£o
- **Airflow UI:** `http://localhost:8080` (admin/admin)
- **MinIO Console:** `http://localhost:9001` (usu√°rio/senha do seu `.env`)
- **Streamlit Dashboard:** Execute com `streamlit run dashboard/app.py` e acesse `http://localhost:8501`.

---

## VI. üìä Resultados e Evid√™ncias

### üóÑÔ∏è Data Lake em A√ß√£o (MinIO)
A implementa√ß√£o no MinIO segue a arquitetura Medallion, com buckets separados para as camadas **Bronze**, **Silver** e **Gold**, demonstrando organiza√ß√£o e governan√ßa.

### üìä Qualidade de Dados Garantida (Great Expectations)
A DAG `dag_05_validacao_segura_v1` demonstra o sucesso na aplica√ß√£o dos Quality Gates, assegurando que apenas dados de alta qualidade progridam no pipeline.

### ‚ö° Pipeline em Execu√ß√£o e M√©tricas de Performance (Airflow UI)
As DAGs no Airflow demonstram orquestra√ß√£o robusta e efici√™ncia, com m√©tricas de performance vis√≠veis na UI para tarefas de **processamento Spark**, **consolida√ß√£o/mascaramento** e **carga no Data Mart**.

### üîê Seguran√ßa Implementada
O framework de seguran√ßa √© validado por:
- **Credenciais Criptografadas:** O arquivo `vault.json` armazena segredos de forma segura.
- **Auditoria Completa:** Os logs em `logs/security_audit/` registram cada opera√ß√£o cr√≠tica.
- **Mascaramento de PII:** A camada Silver cont√©m dados sens√≠veis protegidos, em conformidade com a LGPD.

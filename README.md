# üöÄ Pipeline de Dados Seguro: Da Ingest√£o √† Visualiza√ß√£o Anal√≠tica

![Capa do Projeto](docs/images/pipeline_cover.png)

*Uma arquitetura de engenharia de dados robusta, segura e 100% reprodut√≠vel para ambientes empresariais.*

---

## üìã √çndice

* [I. üéØ Objetivo do Case](#i--objetivo-do-case)
    * [Desafio](#desafio)
    * [Compet√™ncias Demonstradas](#compet√™ncias-demonstradas)
    * [Valor de Neg√≥cio](#valor-de-neg√≥cio)
* [II. üèõÔ∏è Arquitetura da Solu√ß√£o e Arquitetura T√©cnica](#ii--arquitetura-da-solu√ß√£o-e-arquitetura-t√©cnica)
    * [Vis√£o Geral da Arquitetura](#vis√£o-geral-da-arquitetura)
    * [Detalhamento dos Componentes](#detalhamento-dos-componentes)
    * [üîê Framework de Seguran√ßa (Customizado - `plugins/security_system/`)](#-framework-de-seguran√ßa-customizado---pluginssecurity_system)
    * [üóÑÔ∏è Data Lake com Arquitetura Medallion (MinIO)](#-data-lake-com-arquitetura-medallion-minio)
    * [‚ö° Processamento Distribu√≠do (Apache Spark)](#-processamento-distribu√≠do-apache-spark)
    * [üìä Qualidade de Dados (Great Expectations)](#-qualidade-de-dados-great-expectations)
* [III. ‚öôÔ∏è Explica√ß√£o sobre o Case Desenvolvido](#iii--explica√ß√£o-sobre-o-case-desenvolvido)
    * [Fluxo de Trabalho do Pipeline](#fluxo-de-trabalho-do-pipeline)
    * [üîÑ Etapas Detalhadas do Pipeline](#-etapas-detalhadas-do-pipeline)
    * [Fontes de Dados Integradas](#fontes-de-dados-integradas)
* [IV. üß† Melhorias e Considera√ß√µes Finais](#iv--melhorias-e-considera√ß√µes-finais)
    * [Decis√µes de Projeto e Pr√°ticas de Produ√ß√£o](#decis√µes-de-projeto-e-pr√°ticas-de-produ√ß√£o)
    * [üöÄ Melhorias Propostas para Pr√≥ximas Itera√ß√µes](#-melhorias-propostas-para-pr√≥ximas-itera√ß√µes)
    * [üìà Escalabilidade e Performance (Proje√ß√µes)](#-escalabilidade-e-performance-proje√ß√µes)
    * [üèÜ Considera√ß√µes Finais](#-considera√ß√µes-finais)
* [V. üõ†Ô∏è Reprodutibilidade da Arquitetura](#v--reprodutibilidade-da-arquitetura)
    * [Pr√©-requisitos do Sistema](#pr√©-requisitos-do-sistema)
    * [üöÄ Instala√ß√£o e Execu√ß√£o](#-instala√ß√£o-e-execu√ß√£o)
    * [üîç Verifica√ß√£o da Instala√ß√£o](#-verifica√ß√£o-da-instala√ß√£o)
    * [Execu√ß√£o do Pipeline](#execu√ß√£o-do-pipeline)
    * [üêõ Solu√ß√£o de Problemas](#-solu√ß√£o-de-problemas)
    * [üì¶ Estrutura do Projeto](#-estrutura-do-projeto)
    * [‚úÖ Checklist de Valida√ß√£o para a Banca](#-checklist-de-valida√ß√£o-para-a-banca)
* [VI. üìä Resultados e Evid√™ncias](#vi--resultados-e-evid√™ncias)
    * [üóÑÔ∏è Arquitetura de Data Lake em A√ß√£o (MinIO)](#-arquitetura-de-data-lake-em-a√ß√£o-minio)
    * [üìä Qualidade de Dados Garantida (Great Expectations)](#-qualidade-de-dados-garantida-great-expectations)
    * [‚ö° Pipeline em Execu√ß√£o e M√©tricas de Performance (Airflow UI)](#-pipeline-em-execu√ß√£o-e-m√©tricas-de-performance-airflow-ui)
    * [üîê Seguran√ßa Implementada (Framework Customizado)](#-seguran√ßa-implementada-framework-customizado)

---

## I. üéØ Objetivo do Case

### Desafio

O objetivo deste projeto √© demonstrar a constru√ß√£o de um **pipeline de dados ponta a ponta** em uma arquitetura **100% local e open-source**, garantindo total reprodutibilidade. A solu√ß√£o abrange desde a ingest√£o de m√∫ltiplas fontes at√© a cria√ß√£o de um dashboard anal√≠tico interativo, com um foco rigoroso em **seguran√ßa, qualidade, governan√ßa e automa√ß√£o**.

### Compet√™ncias Demonstradas

Este projeto √© uma evid√™ncia pr√°tica de **compet√™ncias avan√ßadas em Engenharia de Dados**, abrangendo:

* **üîß Orquestra√ß√£o de fluxos complexos e resilientes** com Apache Airflow, utilizando DAGs modularizadas.
* **‚ö° Processamento de dados em larga escala** com Apache Spark, incluindo otimiza√ß√µes para ambiente distribu√≠do.
* **üèóÔ∏è Modelagem dimensional e arquitetura Star Schema** para Data Warehouses.
* **üîê Desenvolvimento de um Framework de Seguran√ßa customizado** (principal diferencial), com Vault de segredos criptografado e sistema de auditoria.
* **üìä Implementa√ß√£o de Quality Gates** com Great Expectations para garantir a qualidade dos dados em diferentes est√°gios.
* **üèõÔ∏è Constru√ß√£o de uma arquitetura de Data Lake Medallion** (Bronze, Silver, Gold) com MinIO.
* **ü§ñ Automa√ß√£o de processos** de setup e refatora√ß√£o de c√≥digo, demonstrando um olhar para a efici√™ncia e manutenibilidade.
* **üìà Visualiza√ß√£o anal√≠tica e Business Intelligence** com Streamlit e Grafana.
* **üê≥ Gest√£o de ambientes com Docker e Docker Compose**, garantindo portabilidade e reprodutibilidade.

### Valor de Neg√≥cio

A solu√ß√£o demonstra uma abordagem de engenharia completa, preparada para os desafios e requisitos de seguran√ßa de ambientes de produ√ß√£o. Ela possibilita o processamento seguro de dados heterog√™neos, a gera√ß√£o de insights acion√°veis para tomada de decis√£o empresarial, e serve como um template robusto para pipelines futuros, com foco em compliance e governan√ßa.

---

## II. üèõÔ∏è Arquitetura da Solu√ß√£o e Arquitetura T√©cnica

### Vis√£o Geral da Arquitetura

A arquitetura foi desenhada para ser totalmente contida no ambiente local, utilizando ferramentas open-source que simulam um ecossistema de dados corporativo moderno e robusto.

![Vis√£o Geral da Arquitetura](docs/images/architecture_diagram.png)

> **Nota:** Para o diagrama acima ser exibido, gere uma imagem (PNG/SVG) a partir do c√≥digo Mermaid correspondente e salve-a no caminho `docs/images/architecture_diagram.png`.

### Detalhamento dos Componentes

A solu√ß√£o √© composta por um conjunto de servi√ßos orquestrados via Docker Compose, cada um com uma responsabilidade bem definida:

* **Apache Airflow:** O cora√ß√£o da orquestra√ß√£o. Gerencia o agendamento e a execu√ß√£o de todas as DAGs, que representam os pipelines de dados.
* **PostgreSQL:** Atua como o banco de dados de metadados do Airflow e como o Data Warehouse/Data Mart principal, hospedando o modelo Star Schema.
* **MinIO:** Simula um Object Storage compat√≠vel com S3 (Data Lake), onde os dados s√£o armazenados nas camadas Bronze, Silver e Gold, al√©m de uma camada de Cold Storage.
* **Redis:** Utilizado como broker de mensagens para o Celery Executor do Airflow, permitindo a execu√ß√£o distribu√≠da e escal√°vel de tarefas.
* **Apache Spark:** Motor de processamento distribu√≠do, ideal para transforma√ß√µes e agrega√ß√µes em larga escala, especialmente na transi√ß√£o entre as camadas do Data Lake.
* **Streamlit:** Uma interface simples e poderosa para construir dashboards interativos de Business Intelligence, conectando-se aos dados processados no PostgreSQL.
* **Grafana:** Ferramenta para monitoramento e visualiza√ß√£o de m√©tricas operacionais, podendo ser integrada para acompanhar a sa√∫de dos servi√ßos e do pipeline.

### üîê Framework de Seguran√ßa (Customizado - `plugins/security_system/`)

Este √© o principal diferencial do projeto, demonstrando um profundo conhecimento em seguran√ßa de dados e engenharia de software. Ele garante a integridade, confidencialidade e rastreabilidade dos dados em todo o pipeline:

* **Security Vault** (`VaultManager` em `plugins/security_system/vault_manager_helper.py`): Um cofre digital baseado em arquivo JSON, criptografado com Fernet (AES-128 GCM). Armazena credenciais sens√≠veis de forma centralizada e segura, recuperando-as em tempo de execu√ß√£o.
* **Audit Logger** (`AuditLogger` em `plugins/security_system/audit.py`): Um sistema de auditoria abrangente que registra todas as opera√ß√µes cr√≠ticas do pipeline em um formato estruturado para conformidade (LGPD, SOX) e rastreabilidade.
* **Secure Connection Pool** (`SecureConnectionPool` em `plugins/security_system/secure_connection_pool.py`): Um gerenciador que facilita a obten√ß√£o segura de clientes para servi√ßos externos (MinIO, PostgreSQL), buscando as credenciais do Vault.
* **Exce√ß√µes Customizadas** (`plugins/security_system/exceptions.py`): Uma hierarquia de exce√ß√µes espec√≠ficas para o sistema de seguran√ßa, permitindo tratamento de erros granular e informativo.
* **Rota√ß√£o de Chaves** (`plugins/security_system/key_rotation.py`): M√≥dulo que simula a rota√ß√£o segura de chaves criptogr√°ficas, armazenando vers√µes antigas para descriptografia de dados legados.

### üóÑÔ∏è Data Lake com Arquitetura Medallion (MinIO)

Implementa√ß√£o de um Data Lake multicamadas para governan√ßa e qualidade de dados:

| Camada          | Descri√ß√£o                     | Caracter√≠sticas                                              |
| :-------------- | :---------------------------- | :----------------------------------------------------------- |
| ü•â **Bronze** | Dados Brutos e Imut√°veis      | Raw data, schema-on-read, audit√°vel, imut√°vel.               |
| ü•à **Silver** | Dados Limpos e Padronizados   | LGPD compliant, PII mascarado, deduplicado, pronto para an√°lise. |
| ü•á **Gold** | Dados Agregados e Otimizados  | Regras de neg√≥cio aplicadas, sumarizado, alta performance para BI. |
| üßä **Cold Storage** | Dados Arquivados/Inativos     | Otimiza√ß√£o de custos, acesso menos frequente.                |

### ‚ö° Processamento Distribu√≠do (Apache Spark)

Utilizado para transforma√ß√µes complexas e em larga escala, executando opera√ß√µes de limpeza, normaliza√ß√£o, enriquecimento e agrega√ß√£o. As credenciais para MinIO/S3 s√£o passadas de forma segura ao Spark em tempo de execu√ß√£o via vari√°veis de ambiente, prevenindo o hardcoding. A persist√™ncia em Parquet, formato colunar otimizado para Big Data, √© ideal para efici√™ncia de leitura e escrita.

### üìä Qualidade de Dados (Great Expectations)

Implementa√ß√£o de Quality Gates para garantir a confian√ßa nos dados. S√£o aplicadas su√≠tes de expectativas em datasets em etapas cr√≠ticas. Com uma estrat√©gia "Fail-Fast", os pipelines s√£o interrompidos automaticamente se as expectativas cr√≠ticas de qualidade n√£o forem atendidas, prevenindo a propaga√ß√£o de dados ruins. Os resultados das valida√ß√µes s√£o logados e auditados.

---

## III. ‚öôÔ∏è Explica√ß√£o sobre o Case Desenvolvido

O projeto demonstra um pipeline de dados completo e seguro, orquestrado por uma s√©rie de DAGs no Apache Airflow. Cada DAG possui uma responsabilidade clara e se integra nativamente com o framework de seguran√ßa customizado desenvolvido.

### Fluxo de Trabalho do Pipeline

![Fluxo de Trabalho do Pipeline](docs/images/pipeline_flow.png)

> **Nota:** Para o diagrama acima ser exibido, gere uma imagem (PNG/SVG) a partir do c√≥digo Mermaid correspondente e salve-a no caminho `docs/images/pipeline_flow.png`.

### üîÑ Etapas Detalhadas do Pipeline

#### Coleta Segura (`dag_01_coleta_segura_v1`, `dag_coleta_dados_externos_enterprise_v1`)

* **Objetivo:** Ingest√£o inicial de dados brutos de fontes externas.
* **Fontes:** APIs externas (Banco Central para IPCA, OpenWeatherMap para clima) e datasets locais (Olist).
* **Seguran√ßa:** Credenciais (ex: API Key do OpenWeatherMap) s√£o obtidas do Security Vault em tempo de execu√ß√£o.
* **Destino:** Dados brutos persistidos na camada **Bronze** do MinIO.
* **Diferencial:** Demonstra o uso de `PythonOperator` para ingest√£o, `Requests` para APIs, e integra√ß√£o com o framework de seguran√ßa.

#### Consolida√ß√£o e Mascaramento PII (`dag_03_consolidacao_e_mascaramento_v1`)

* **Objetivo:** Limpar, unificar e proteger dados sens√≠veis.
* **Processo:** Dados lidos da camada Bronze e unificados via `pandas.merge`.
* **Seguran√ßa PII:** Aplica mascaramento de informa√ß√µes pessoalmente identific√°veis (PII), garantindo LGPD/GDPR compliance.
* **Destino:** Dados limpos e mascarados persistidos na camada **Silver** do MinIO.
* **Diferencial:** Implementa√ß√£o pr√°tica de t√©cnicas de Data Privacy (`DataProtection` module) e auditoria detalhada das transforma√ß√µes.

#### Processamento em Larga Escala (`dag_04_processamento_spark_seguro_v1`)

* **Objetivo:** Transformar dados da camada Silver em dados agregados e otimizados para BI.
* **Processo:** Um job Spark √© submetido pelo Airflow, processando dados do MinIO/S3.
* **Seguran√ßa:** Credenciais para acesso ao MinIO/S3 s√£o injetadas de forma segura no ambiente do Spark em tempo de execu√ß√£o, diretamente do Security Vault.
* **Destino:** Gera√ß√£o da camada **Gold** no MinIO, com dados sumarizados e prontos para consumo anal√≠tico.
* **Diferencial:** Demonstra o uso de `BashOperator` para `spark-submit`, passando credenciais de forma segura via ambiente.

#### Valida√ß√£o de Qualidade (`dag_05_validacao_segura_v1`, `scripts/examples/19-validacao_great_expectations_avancada.py`)

* **Objetivo:** Assegurar a integridade e consist√™ncia dos dados antes de seu consumo final.
* **Processo:** Aplica√ß√£o de uma su√≠te de expectativas de qualidade (Great Expectations) nos dados da camada Gold.
* **Qualidade:** Atua como um "Quality Gate", falhando a DAG se os dados n√£o atenderem aos crit√©rios m√≠nimos.
* **Auditoria:** Os resultados das valida√ß√µes s√£o registrados detalhadamente no Audit Logger.

#### Carga no Data Mart (`dag_06_carrega_star_schema_segura_enterprise_v1`, `dag_minio_para_postgresql_enterprise_v1`)

* **Objetivo:** Carregar dados da camada Gold do Data Lake para o Data Mart relacional.
* **Modelo:** Popula√ß√£o de um modelo dimensional Star Schema no PostgreSQL, dentro de transa√ß√µes ACID.
* **Seguran√ßa:** Conex√£o segura ao PostgreSQL utilizando credenciais obtidas do Security Vault.
* **Diferencial:** Demonstra ETL de Data Lake para Data Mart, transa√ß√µes ACID, e uso de `SecureConnectionPool`.

#### Gerenciamento de Lifecycle (`dag_gerenciamento_lifecycle_enterprise_v1`)

* **Objetivo:** Otimizar custos de armazenamento e gerenciar a reten√ß√£o de dados.
* **Processo:** Move automaticamente arquivos antigos da camada Bronze para uma camada de Cold Storage (simulada no MinIO).
* **Seguran√ßa:** Opera√ß√µes de movimenta√ß√£o de dados s√£o autenticadas via Security Vault e auditadas.

### Fontes de Dados Integradas

| Fonte          | Tipo        | Descri√ß√£o                                         | Volume Simulado |
| :------------- | :---------- | :------------------------------------------------ | :-------------- |
| **Banco Central** | API REST    | Indicadores econ√¥micos (IPCA)                     | Pequeno         |
| **OpenWeather** | API REST    | Dados meteorol√≥gicos por regi√£o (temperatura, etc.) | Pequeno         |
| **Olist** | Dataset CSV | Dados reais de e-commerce brasileiro (p√∫blico)    | Grande          |

---

## IV. üß† Melhorias e Considera√ß√µes Finais

### Decis√µes de Projeto e Pr√°ticas de Produ√ß√£o

* **Modularidade e Reuso de Componentes de Seguran√ßa:** O framework de seguran√ßa (`security_system`) foi projetado como um conjunto de m√≥dulos reutiliz√°veis. A separa√ß√£o entre `VaultManager` (l√≥gica de segredos) e `AirflowSecurityManager` (integra√ß√£o UI) √© crucial para clareza e manuten√ß√£o em um ambiente enterprise.
* **Configura√ß√£o de Credenciais:** No Case, para fins de reprodutibilidade, as credenciais s√£o lidas de vari√°veis de ambiente do `.env`. Em Produ√ß√£o, a `SECURITY_VAULT_SECRET_KEY` viria de um servi√ßo como AWS Secrets Manager ou HashiCorp Vault, nunca de um arquivo `.env` versionado.
* **Automa√ß√£o da Refatora√ß√£o (`refinar_projeto.py`):** Foi desenvolvido um script para automatizar a adapta√ß√£o de caminhos e a inser√ß√£o de valida√ß√µes. Isso mostra uma mentalidade de engenharia que busca resolver problemas de forma program√°tica, uma pr√°tica essencial em equipes de alta performance.

### üöÄ Melhorias Propostas para Pr√≥ximas Itera√ß√µes

Para evoluir este projeto para um n√≠vel de produ√ß√£o ainda mais avan√ßado, as seguintes melhorias s√£o propostas:

* **Infraestrutura como C√≥digo (IaC):**
    * **Terraform:** Para automa√ß√£o da provis√£o de recursos em nuvem (ex: EC2, RDS, S3, EMR).
    * **Ansible:** Para configura√ß√£o e deploy de aplica√ß√µes nos servidores.
* **CI/CD para Pipelines:**
    * **GitHub Actions / GitLab CI:** Para automa√ß√£o de testes unit√°rios, de integra√ß√£o e end-to-end.
    * **Deploy Automatizado:** Novas vers√µes de DAGs seriam implantadas automaticamente.
* **Cat√°logo de Dados:**
    * Integra√ß√£o com **Apache Atlas** ou **Amundsen** para documenta√ß√£o autom√°tica de metadados e linhagem de dados.
* **Observabilidade Avan√ßada:**
    * M√©tricas Customizadas com **Prometheus / Grafana** para monitoramento granular.
    * Distributed Tracing com **Jaeger / OpenTelemetry** para depura√ß√£o de pipelines complexos.

### üìà Escalabilidade e Performance (Proje√ß√µes)

| Aspecto         | Implementa√ß√£o Atual (Local Docker)  | Melhoria Proposta (Cloud / Otimiza√ß√µes)               |
| :-------------- | :---------------------------------- | :---------------------------------------------------- |
| **Volume** | ~100k registros Olist               | **Petabytes** (particionamento horizontal, sharding)  |
| **Lat√™ncia** | < 30 segundos (pipeline end-to-end) | **< 10 segundos** (para ingest√£o, com Kafka/Redis)    |
| **Concorr√™ncia**| 3 DAGs paralelas (com CeleryExecutor) | **10+ DAGs e tasks simult√¢neas** (Kubernetes, Fargate) |
| **Monitoramento**| Logs b√°sicos, Airflow UI            | **APM completo**, dashboards Grafana, alertas SMS/email |
| **Persist√™ncia**| Volumes Docker, MinIO local         | **S3/GCS/Azure Blob Storage**, Databases gerenciados  |

### üèÜ Considera√ß√µes Finais

Este case entrega uma solu√ß√£o de dados enterprise-grade, segura, confi√°vel, escal√°vel e totalmente reprodut√≠vel. As decis√µes de projeto, como a cria√ß√£o de um framework de seguran√ßa customizado e a automa√ß√£o de tarefas de desenvolvimento/deploy, demonstram um dom√≠nio de conceitos que v√£o muito al√©m do b√°sico, focando nos desafios reais de um ambiente corporativo moderno.

A arquitetura implementada √© *production-ready* em seus princ√≠pios e pode ser facilmente adaptada e estendida para ambientes de nuvem em grande escala, mantendo os mesmos pilares de seguran√ßa, qualidade e governan√ßa.

---

## V. üõ†Ô∏è Reprodutibilidade da Arquitetura

Este projeto foi constru√≠do com foco na reprodutibilidade e portabilidade, utilizando Docker para isolar o ambiente. As instru√ß√µes a seguir detalham como configurar e executar o projeto.

### Pr√©-requisitos do Sistema

* **Softwares Necess√°rios**
    * Python 3.8+ (com pip)
    * Git (vers√£o 2.25+)
    * Docker e Docker Compose
* **Recursos de Hardware M√≠nimos**
    * RAM: 8GB (recomendado 16GB)
    * Armazenamento: 10GB livres
    * CPU: 4 cores (recomendado 8 cores)

### üöÄ Instala√ß√£o e Execu√ß√£o

Siga os passos rigorosamente para garantir a correta inicializa√ß√£o do ambiente.

**Passo 1: Clonagem do Reposit√≥rio**
```bash
git clone [https://github.com/felipesbonatti/case-data-master-engenharia-de-dados.git](https://github.com/felipesbonatti/case-data-master-engenharia-de-dados.git)
cd case-data-master-engenharia-de-dados

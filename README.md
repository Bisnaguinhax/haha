Pipeline de Dados Seguro: Da IngestÃ£o Ã  VisualizaÃ§Ã£o AnalÃ­ticağŸ“‹ ÃndiceI. Objetivo do CaseII. Arquitetura da SoluÃ§Ã£o e Arquitetura TÃ©cnicaIII. ExplicaÃ§Ã£o sobre o Case DesenvolvidoIV. Melhorias e ConsideraÃ§Ãµes FinaisV. Reprodutibilidade da ArquiteturaVI. Resultados e EvidÃªnciasI. ğŸ¯ Objetivo do CaseDesafioO objetivo deste projeto Ã© demonstrar a construÃ§Ã£o de um pipeline de dados ponta a ponta em uma arquitetura 100% local e open-source, garantindo total reprodutibilidade. A soluÃ§Ã£o abrange desde a ingestÃ£o de mÃºltiplas fontes atÃ© a criaÃ§Ã£o de um dashboard analÃ­tico interativo, com um foco rigoroso em seguranÃ§a, qualidade, governanÃ§a e automaÃ§Ã£o.CompetÃªncias DemonstradasEste projeto Ã© uma evidÃªncia prÃ¡tica de competÃªncias avanÃ§adas em Engenharia de Dados, abrangendo:ğŸ”§ OrquestraÃ§Ã£o de fluxos complexos e resilientes com Apache Airflow, utilizando DAGs modularizadas.âš¡ Processamento de dados em larga escala com Apache Spark, incluindo otimizaÃ§Ãµes para ambiente distribuÃ­do.ğŸ—ï¸ Modelagem dimensional e arquitetura Star Schema para Data Warehouses.ğŸ” Desenvolvimento de um Framework de SeguranÃ§a customizado (principal diferencial), com Vault de segredos criptografado e sistema de auditoria.ğŸ“Š ImplementaÃ§Ã£o de Quality Gates com Great Expectations para garantir a qualidade dos dados em diferentes estÃ¡gios.ğŸ›ï¸ ConstruÃ§Ã£o de uma arquitetura de Data Lake Medallion (Bronze, Silver, Gold) com MinIO.ğŸ¤– AutomaÃ§Ã£o de processos de setup e refatoraÃ§Ã£o de cÃ³digo, demonstrando um olhar para a eficiÃªncia e manutenibilidade.ğŸ“ˆ VisualizaÃ§Ã£o analÃ­tica e Business Intelligence com Streamlit e Grafana.ğŸ³ GestÃ£o de ambientes com Docker e Docker Compose, garantindo portabilidade e reprodutibilidade.Valor de NegÃ³cioA soluÃ§Ã£o demonstra uma abordagem de engenharia completa, preparada para os desafios e requisitos de seguranÃ§a de ambientes de produÃ§Ã£o. Ela possibilita o processamento seguro de dados heterogÃªneos, a geraÃ§Ã£o de insights acionÃ¡veis para tomada de decisÃ£o empresarial, e serve como um template robusto para pipelines futuros, com foco em compliance e governanÃ§a.II. ğŸ›ï¸ Arquitetura da SoluÃ§Ã£o e Arquitetura TÃ©cnicaVisÃ£o Geral da ArquiteturaA arquitetura foi desenhada para ser totalmente contida no ambiente local, utilizando ferramentas open-source que simulam um ecossistema de dados corporativo moderno e robusto.graph TD
    subgraph "ğŸŒ Fontes de Dados"
        API1[API Banco Central<br/>IPCA]
        API2[API OpenWeather<br/>Dados ClimÃ¡ticos]
        DS1[Dataset Olist<br/>E-commerce]
    end
    
    subgraph "ğŸ¯ OrquestraÃ§Ã£o"
        AF[Apache Airflow<br/>DAGs Modularizadas]
    end

    subgraph "ğŸ” Camada de SeguranÃ§a"
        VAULT[Security Vault<br/>AES-128 Encryption]
        AUDIT[Audit Logger<br/>Rastreabilidade]
        CONN[Secure Connection Pool<br/>Runtime Credentials]
    end
    
    subgraph "ğŸ—„ï¸ Data Lake (MinIO)"
        BRONZE[Bronze Layer<br/>Raw Data]
        SILVER[Silver Layer<br/>Cleansed + PII Masked]
        GOLD[Gold Layer<br/>Aggregated]
        COLD[Cold Storage Layer<br/>Archived Data]
    end
    
    subgraph "âš¡ Processamento & Qualidade"
        SPARK[Apache Spark<br/>Distributed Processing]
        GE[Great Expectations<br/>Quality Gates]
    end
    
    subgraph "ğŸ›ï¸ Data Warehouse"
        PG[(PostgreSQL<br/>Star Schema)]
    end
    
    subgraph "ğŸ“Š VisualizaÃ§Ã£o & Monitoramento"
        ST[Streamlit Dashboard<br/>Interactive Analytics]
        GF[Grafana<br/>Monitoring & Alerts]
    end
    
    API1 --> AF
    API2 --> AF
    DS1 --> AF

    AF --> VAULT
    AF --> AUDIT
    AF --> CONN

    AF --> BRONZE
    BRONZE --> SPARK
    SPARK --> SILVER
    SILVER --> SPARK
    SPARK --> GOLD
    GOLD --> GE
    GE --> PG
    PG --> ST
    PG --> GF
    
    VAULT --> CONN
    CONN --> SPARK
    CONN --> PG

    AUDIT -.->|Logs| AF
    AF -.->|Health Checks| PG
    AF -.->|Health Checks| MinIO_Container
    AF -.->|Health Checks| Redis_Container
    MinIO_Container[MinIO Service] --> BRONZE
    MinIO_Container --> SILVER
    MinIO_Container --> GOLD
    MinIO_Container --> COLD

    BRONZE --> COLD[MovimentaÃ§Ã£o de Lifecycle]
Detalhamento dos ComponentesA soluÃ§Ã£o Ã© composta por um conjunto de serviÃ§os orquestrados via Docker Compose, cada um com uma responsabilidade bem definida:Apache Airflow: O coraÃ§Ã£o da orquestraÃ§Ã£o. Gerencia o agendamento e a execuÃ§Ã£o de todas as DAGs, que representam os pipelines de dados.PostgreSQL: Atua como o banco de dados de metadados do Airflow e como o Data Warehouse/Data Mart principal, hospedando o modelo Star Schema.MinIO: Simula um Object Storage compatÃ­vel com S3 (Data Lake), onde os dados sÃ£o armazenados nas camadas Bronze, Silver e Gold, alÃ©m de uma camada de Cold Storage.Redis: Utilizado como broker de mensagens para o Celery Executor do Airflow, permitindo a execuÃ§Ã£o distribuÃ­da e escalÃ¡vel de tarefas.Apache Spark: Motor de processamento distribuÃ­do, ideal para transformaÃ§Ãµes e agregaÃ§Ãµes em larga escala, especialmente na transiÃ§Ã£o entre as camadas do Data Lake.Streamlit: Uma interface simples e poderosa para construir dashboards interativos de Business Intelligence, conectando-se aos dados processados no PostgreSQL.Grafana: Ferramenta para monitoramento e visualizaÃ§Ã£o de mÃ©tricas operacionais, podendo ser integrada para acompanhar a saÃºde dos serviÃ§os e do pipeline.ğŸ” Framework de SeguranÃ§a (Customizado - plugins/security_system/)Este Ã© o principal diferencial do projeto, demonstrando um profundo conhecimento em seguranÃ§a de dados e engenharia de software. Ele garante a integridade, confidencialidade e rastreabilidade dos dados em todo o pipeline:Security Vault (VaultManager em plugins/security_system/vault_manager_helper.py): Um cofre digital baseado em arquivo JSON, criptografado com Fernet (AES-128 GCM). Armazena credenciais sensÃ­veis (APIs externas, MinIO, PostgreSQL) de forma centralizada e segura, recuperando-as em tempo de execuÃ§Ã£o.Diferencial: SeparaÃ§Ã£o clara de responsabilidades da AirflowSecurityManager (integraÃ§Ã£o UI do Airflow) e VaultManager (gestÃ£o real de segredos).Audit Logger (AuditLogger em plugins/security_system/audit.py): Um sistema de auditoria abrangente que registra todas as operaÃ§Ãµes crÃ­ticas do pipeline (acessos a segredos, uploads, transformaÃ§Ãµes, validaÃ§Ãµes, incidentes de seguranÃ§a) em um formato estruturado para conformidade (LGPD, SOX) e rastreabilidade.Secure Connection Pool (SecureConnectionPool em plugins/security_system/secure_connection_pool.py): Um gerenciador que facilita a obtenÃ§Ã£o segura de clientes para serviÃ§os externos (MinIO, PostgreSQL), buscando as credenciais do Vault e garantindo que nunca sejam hardcoded.ExceÃ§Ãµes Customizadas (plugins/security_system/exceptions.py): Uma hierarquia de exceÃ§Ãµes especÃ­ficas para o sistema de seguranÃ§a, permitindo tratamento de erros granular e informativo.RotaÃ§Ã£o de Chaves (plugins/security_system/key_rotation.py): MÃ³dulo que simula a rotaÃ§Ã£o segura de chaves criptogrÃ¡ficas, armazenando versÃµes antigas para descriptografia de dados legados.ğŸ—„ï¸ Data Lake com Arquitetura Medallion (MinIO)ImplementaÃ§Ã£o de um Data Lake multicamadas para governanÃ§a e qualidade de dados:CamadaDescriÃ§Ã£oCaracterÃ­sticasBronzeDados Brutos e ImutÃ¡veis: IngestÃ£o direta de fontes.Raw data, schema-on-read, auditÃ¡vel, imutÃ¡vel.SilverDados Limpos e Padronizados: PII mascarado, validado.LGPD compliant, deduplicado, pronto para anÃ¡lise exploratÃ³ria.GoldDados Agregados e Otimizados: Para consumo de BI.Regras de negÃ³cio aplicadas, sumarizado, alta performance.Cold StorageDados Arquivados/Inativos: Para retenÃ§Ã£o de longo prazo.OtimizaÃ§Ã£o de custos, acesso menos frequente.âš¡ Processamento DistribuÃ­do (Apache Spark)Utilizado para transformaÃ§Ãµes complexas e em larga escala:Processamento na TransiÃ§Ã£o Bronze -> Silver -> Gold: Executa operaÃ§Ãµes de limpeza, normalizaÃ§Ã£o, enriquecimento e agregaÃ§Ã£o.InjeÃ§Ã£o Segura de Credenciais: Credenciais para MinIO/S3 sÃ£o passadas de forma segura ao Spark em tempo de execuÃ§Ã£o via variÃ¡veis de ambiente, prevenindo o hardcoding.PersistÃªncia em Parquet: Formato colunar otimizado para Big Data, ideal para eficiÃªncia de leitura e escrita.ğŸ“Š Qualidade de Dados (Great Expectations)ImplementaÃ§Ã£o de Quality Gates para garantir a confianÃ§a nos dados:ValidaÃ§Ã£o em Etapas CrÃ­ticas: AplicaÃ§Ã£o de suÃ­tes de expectativas em datasets (ex: apÃ³s consolidaÃ§Ã£o ou antes da carga no Data Warehouse).Fail-Fast Strategy: Pipelines sÃ£o interrompidos automaticamente se as expectativas crÃ­ticas de qualidade nÃ£o forem atendidas, prevenindo a propagaÃ§Ã£o de dados ruins.Rastreabilidade: Os resultados das validaÃ§Ãµes sÃ£o logados e auditados.III. âš™ï¸ ExplicaÃ§Ã£o sobre o Case DesenvolvidoO projeto demonstra um pipeline de dados completo e seguro, orquestrado por uma sÃ©rie de DAGs no Apache Airflow. Cada DAG possui uma responsabilidade clara e se integra nativamente com o framework de seguranÃ§a customizado desenvolvido.Fluxo de Trabalho do Pipelineflowchart TD
    subgraph "OrquestraÃ§Ã£o Airflow"
        A[1. Coleta Segura<br/>(IPCA, Clima, Olist)] --> B[2. ConsolidaÃ§Ã£o e Mascaramento PII<br/>(Olist)]
        B --> C[3. Processamento Spark<br/>(Silver -> Gold)]
        C --> D[4. ValidaÃ§Ã£o Qualidade<br/>(Great Expectations)]
        D --> E[5. Carga no Data Mart<br/>(Star Schema - PostgreSQL)]
        E --> F[6. Gerenciamento de Lifecycle<br/>(MinIO Cold Storage)]
    end

    F --> G[7. Dashboard de BI<br/>(Streamlit)]
    E --> H[8. Monitoramento Operacional<br/>(Grafana)]

    subgraph "Camada de SeguranÃ§a Integrada"
        Vault_Core[Security Vault<br/>(VaultManager)]
        Audit_Core[Audit Logger]
    end

    A -- Credenciais de API --> Vault_Core
    Vault_Core --> B
    B -- Dados Mascarados --> C
    C -- Dados Processados --> D
    D -- Dados Validados --> E
    E -- Dados Carregados --> F
    
    A, B, C, D, E, F -.-> Audit_Core[Logs de Auditoria]
    A, B, C, D, E, F -.-> Airflow_UI[Monitoramento Airflow UI]
ğŸ”„ Etapas Detalhadas do PipelineColeta Segura (dag_01_coleta_segura_v1, dag_coleta_dados_externos_enterprise_v1)Objetivo: IngestÃ£o inicial de dados brutos de fontes externas.Fontes: APIs externas (Banco Central para IPCA, OpenWeatherMap para clima) e datasets locais (Olist).SeguranÃ§a: Credenciais (ex: API Key do OpenWeatherMap) sÃ£o obtidas do Security Vault em tempo de execuÃ§Ã£o.Destino: Dados brutos persistidos na camada Bronze do MinIO.Diferencial: Demonstra o uso de PythonOperator para ingestÃ£o, Requests para APIs, e integraÃ§Ã£o com o framework de seguranÃ§a.Exemplo de Log de Sucesso (Coleta e ValidaÃ§Ã£o):ConsolidaÃ§Ã£o e Mascaramento PII (dag_03_consolidacao_e_mascaramento_v1)Objetivo: Limpar, unificar e proteger dados sensÃ­veis.Processo: Dados lidos da camada Bronze e unificados via pandas.merge.SeguranÃ§a PII: Aplica mascaramento de informaÃ§Ãµes pessoalmente identificÃ¡veis (PII) como customer_city (estÃ¡tico) e customer_state (hash), garantindo LGPD/GDPR compliance.Destino: Dados limpos e mascarados persistidos na camada Silver do MinIO.Diferencial: ImplementaÃ§Ã£o prÃ¡tica de tÃ©cnicas de Data Privacy (DataProtection module) e auditoria detalhada das transformaÃ§Ãµes.Exemplo de GrÃ¡fico de DuraÃ§Ã£o (ConsolidaÃ§Ã£o e Mascaramento):Processamento em Larga Escala (dag_04_processamento_spark_seguro_v1)Objetivo: Transformar dados da camada Silver em dados agregados e otimizados para BI.Processo: Um job Spark Ã© submetido pelo Airflow, processando dados do MinIO/S3.SeguranÃ§a: Credenciais para acesso ao MinIO/S3 (camadas Silver/Gold) sÃ£o injetadas de forma segura no ambiente do Spark em tempo de execuÃ§Ã£o, diretamente do Security Vault.Destino: GeraÃ§Ã£o da camada Gold no MinIO, com dados sumarizados e prontos para consumo analÃ­tico (ex: vendas por categoria).Diferencial: Demonstra o uso de BashOperator para spark-submit, passando credenciais de forma segura via ambiente, e configuraÃ§Ãµes robustas de conectividade S3A.Exemplo de GrÃ¡fico de DuraÃ§Ã£o (Processamento Spark):ValidaÃ§Ã£o de Qualidade (dag_05_validacao_segura_v1, scripts/examples/19-validacao_great_expectations_avancada.py)Objetivo: Assegurar a integridade e consistÃªncia dos dados antes de seu consumo final.Processo: AplicaÃ§Ã£o de uma suÃ­te de expectativas de qualidade (Great Expectations) nos dados da camada Gold.Qualidade: Atua como um "Quality Gate", falhando a DAG se os dados nÃ£o atenderem aos critÃ©rios mÃ­nimos.Auditoria: Os resultados das validaÃ§Ãµes sÃ£o registrados detalhadamente no Audit Logger.Diferencial: Uso de uma ferramenta de ponta para qualidade de dados, integrada ao fluxo de orquestraÃ§Ã£o e auditoria.Carga no Data Mart (dag_06_carrega_star_schema_segura_enterprise_v1, dag_minio_para_postgresql_enterprise_v1)Objetivo: Carregar dados da camada Gold do Data Lake para o Data Mart relacional.Modelo: PopulaÃ§Ã£o de um modelo dimensional Star Schema (dimensÃµes de cliente e produto, e tabela de fato de vendas) no PostgreSQL.Transacional: As cargas sÃ£o realizadas dentro de transaÃ§Ãµes ACID (Atomicity, Consistency, Isolation, Durability) para garantir a integridade dos dados.SeguranÃ§a: ConexÃ£o segura ao PostgreSQL utilizando credenciais obtidas do Security Vault.Diferencial: Demonstra ETL de Data Lake para Data Mart, transaÃ§Ãµes ACID, e uso de SecureConnectionPool para gerenciar conexÃµes com credenciais seguras.Exemplo de Grafo da DAG (MinIO para PostgreSQL):Exemplo de Duracao de Task (Carga Star Schema):Gerenciamento de Lifecycle (dag_gerenciamento_lifecycle_enterprise_v1)Objetivo: Otimizar custos de armazenamento e gerenciar a retenÃ§Ã£o de dados.Processo: Move automaticamente arquivos antigos da camada Bronze para uma camada de Cold Storage (simulada no MinIO).SeguranÃ§a: OperaÃ§Ãµes de movimentaÃ§Ã£o de dados sÃ£o autenticadas via Security Vault e auditadas.Diferencial: Abordagem prÃ¡tica para governanÃ§a de dados e otimizaÃ§Ã£o de infraestrutura.Exemplo de Logs (Gerenciamento de Lifecycle):VisualizaÃ§Ã£o de InsightsDashboard Streamlit: Conecta-se ao PostgreSQL para exibir KPIs e anÃ¡lises de vendas, permitindo a exploraÃ§Ã£o de dados por stakeholders.Grafana: Pode ser integrado para monitoramento de mÃ©tricas operacionais e alertas.Fontes de Dados IntegradasFonteTipoDescriÃ§Ã£oSimulaÃ§Ã£o de VolumeBanco CentralAPI RESTIndicadores econÃ´micos (IPCA)Pequeno VolumeOpenWeatherAPI RESTDados meteorolÃ³gicos por regiÃ£o (temperatura, etc.)Pequeno VolumeOlistDataset CSVDados reais de e-commerce brasileiro (pÃºblico)Grande VolumeIV. ğŸ§  Melhorias e ConsideraÃ§Ãµes FinaisDecisÃµes de Projeto e PrÃ¡ticas de ProduÃ§Ã£oÃ‰ fundamental que as decisÃµes de projeto reflitam a consciÃªncia sobre as diferenÃ§as entre um ambiente de demonstraÃ§Ã£o/desenvolvimento e um sistema produtivo em larga escala.Modularidade e Reuso de Componentes de SeguranÃ§a: O framework de seguranÃ§a (security_system nos plugins) foi projetado para ser um conjunto de mÃ³dulos reutilizÃ¡veis.VaultManager vs. AirflowSecurityManager: Mantivemos a AirflowSecurityManager (plugins/security_system/vault.py) focada em estender a seguranÃ§a da UI do Airflow. A lÃ³gica de gestÃ£o real de segredos (criptografia, leitura/escrita no vault.json) foi delegada ao VaultManager (plugins/security_system/vault_manager_helper.py). Essa separaÃ§Ã£o Ã© crucial para clareza e manutenÃ§Ã£o em um ambiente enterprise.Reuso: Em vez de replicar a lÃ³gica de os.getenv('SECRET_KEY') e instanciaÃ§Ã£o do Vault em cada DAG/script, o VaultManager encapsula isso, e as DAGs/scripts apenas o instanciam e o utilizam.ConfiguraÃ§Ã£o de Credenciais:No Case (Demo): Para fins de reprodutibilidade e simplicidade da demonstraÃ§Ã£o, as credenciais para serviÃ§os como MinIO e PostgreSQL sÃ£o lidas de variÃ¡veis de ambiente do .env (populadas via docker-compose.yml) e entÃ£o inseridas no Security Vault via scripts/setup_vault_secrets.py.Em ProduÃ§Ã£o: Em um ambiente real, a SECURITY_VAULT_SECRET_KEY viria de um serviÃ§o de gerenciamento de segredos da nuvem (ex: AWS Secrets Manager, HashiCorp Vault Server) ou de um sistema de orquestraÃ§Ã£o de segredos, nunca de um arquivo .env versionado ou acessÃ­vel diretamente. O setup_vault_secrets.py seria parte de um processo de deploy seguro.AutomaÃ§Ã£o da RefatoraÃ§Ã£o (refinar_projeto.py):No Case: Um script Python foi desenvolvido para automatizar a adaptaÃ§Ã£o de caminhos hardcoded e a inserÃ§Ã£o de blocos de validaÃ§Ã£o de seguranÃ§a em outros arquivos Python.Valor Demonstrado: Isso mostra uma mentalidade de engenharia que busca resolver problemas de forma programÃ¡tica, aumenta a produtividade e reduz erros, uma prÃ¡tica essencial em equipes de alta performance.ğŸš€ Melhorias Propostas para PrÃ³ximas IteraÃ§ÃµesPara evoluir este projeto para um nÃ­vel de produÃ§Ã£o ainda mais avanÃ§ado e escalÃ¡vel, as seguintes melhorias sÃ£o propostas:Infraestrutura como CÃ³digo (IaC)Terraform: Para automatizaÃ§Ã£o completa da provisÃ£o de recursos em nuvem (ex: EC2, RDS, S3, EMR, MWAA).Ansible: Para configuraÃ§Ã£o e automaÃ§Ã£o de deploy de aplicaÃ§Ãµes nos servidores.CI/CD para PipelinesGitHub Actions / GitLab CI: Para automaÃ§Ã£o de testes unitÃ¡rios, de integraÃ§Ã£o e end-to-end.Deploy Automatizado: Novas versÃµes de DAGs e cÃ³digos seriam implantadas automaticamente em ambientes de teste e produÃ§Ã£o.Testes de IntegraÃ§Ã£o Automatizados: CenÃ¡rios de teste que validam o fluxo de dados entre mÃºltiplos componentes.CatÃ¡logo de DadosIntegraÃ§Ã£o com Apache Atlas ou Amundsen: Para documentaÃ§Ã£o automÃ¡tica de metadados, linhagem de dados (data lineage) e descoberta de dados.Observabilidade AvanÃ§adaMÃ©tricas Customizadas com Prometheus / Grafana: Para monitoramento granular de performance do pipeline e recursos da infraestrutura.Alertas Proativos: ConfiguraÃ§Ã£o de alertas em caso de anomalias ou falhas crÃ­ticas.Distributed Tracing com Jaeger / OpenTelemetry: Para depuraÃ§Ã£o de pipelines complexos em ambientes distribuÃ­dos.ğŸ“ˆ Escalabilidade e Performance (ProjeÃ§Ãµes)AspectoImplementaÃ§Ã£o Atual (Local Docker)Melhoria Proposta (Cloud / OtimizaÃ§Ãµes)Volume~100k registros OlistPetabytes (particionamento horizontal, sharding)LatÃªncia< 30 segundos (pipeline end-to-end)< 10 segundos (para ingestÃ£o, com Kafka/Redis)ConcorrÃªncia3 DAGs paralelas (com CeleryExecutor)10+ DAGs e tasks simultÃ¢neas (Kubernetes, Fargate)MonitoramentoLogs bÃ¡sicos, Airflow UIAPM completo, dashboards Grafana, alertas SMS/emailPersistÃªnciaVolumes Docker, MinIO localS3/GCS/Azure Blob Storage, Databases gerenciadosğŸ† ConsideraÃ§Ãµes FinaisEste case entrega uma soluÃ§Ã£o de dados enterprise-grade, segura, confiÃ¡vel, escalÃ¡vel e totalmente reprodutÃ­vel. As decisÃµes de projeto, como a criaÃ§Ã£o de um framework de seguranÃ§a customizado e a automaÃ§Ã£o de tarefas de desenvolvimento/deploy, demonstram um domÃ­nio de conceitos que vÃ£o muito alÃ©m do bÃ¡sico, focando nos desafios reais de um ambiente corporativo moderno.A arquitetura implementada Ã© production-ready em seus princÃ­pios e pode ser facilmente adaptada e estendida para ambientes de nuvem em grande escala, mantendo os mesmos pilares de seguranÃ§a, qualidade e governanÃ§a.V. ğŸ› ï¸ Reprodutibilidade da ArquiteturaEste projeto foi construÃ­do com foco na reprodutibilidade e portabilidade, utilizando Docker para isolar o ambiente. As instruÃ§Ãµes a seguir detalham como configurar e executar o projeto em qualquer mÃ¡quina (macOS, Linux, Windows) com Docker instalado.PrÃ©-requisitos do SistemaSoftwares NecessÃ¡riosPython 3.8+ (com pip)Git (versÃ£o 2.25+)Docker Desktop (para Windows/macOS) ou Docker Engine (para Linux)Docker Compose (geralmente incluÃ­do com o Docker Desktop/Engine)Apache Spark 3.5+: Embora as DAGs rodem o Spark via spark-submit dentro do contÃªiner Airflow, ter o Spark instalado localmente pode ser Ãºtil para depuraÃ§Ã£o ou execuÃ§Ã£o de scripts Spark standalone.macOS: brew install apache-sparkLinux/Windows: Download oficialRecursos de Hardware MÃ­nimosRAM: 8GB (recomendado 16GB para performance ideal com Spark e Airflow)Armazenamento: 10GB livresCPU: 4 cores (recomendado 8 cores)ğŸš€ InstalaÃ§Ã£o e ExecuÃ§Ã£oSiga os passos rigorosamente para garantir a correta inicializaÃ§Ã£o do ambiente.Passo 1: Clonagem do RepositÃ³rioAbra seu terminal e execute:# Clone o repositÃ³rio
git clone https://github.com/felipesbonatti/case-data-master-engenharia-de-dados.git
cd case-data-master-engenharia-de-dados

# Verifique a estrutura do projeto
# (O comando 'tree' pode nÃ£o estar disponÃ­vel no Windows por padrÃ£o, use 'dir /s /b' ou explore manualmente)
tree -L 2
Passo 2: ConfiguraÃ§Ã£o do AmbienteEste passo Ã© CRÃTICO para a seguranÃ§a do Vault e o funcionamento do pipeline.# Crie o arquivo de ambiente (.env) a partir do template
cp .env.example .env

# Gere uma chave de criptografia segura para o Vault e adicione ao .env
# Esta chave Ã© fundamental para criptografar/decriptar seus segredos no Vault.
python -c "from cryptography.fernet import Fernet; print('SECURITY_VAULT_SECRET_KEY=' + Fernet.generate_key().decode())" >> .env

# Abra o arquivo .env e configure suas API keys reais e senhas fortes.
# EXEMPLO DE .env (SUBSTITUA OS VALORES PADRÃƒO POR SEUS VALORES REAIS E SEGUROS):
# SECURITY_VAULT_SECRET_KEY=SUA_CHAVE_FERNET_GERADA_AQUI
# OPENWEATHER_API_KEY=SUA_CHAVE_DA_OPENWEATHERMAP_AQUI
# POSTGRES_USER=airflow_user_real
# POSTGRES_PASSWORD=sua_senha_segura_postgres
# MINIO_ROOT_USER=minio_admin_real
# MINIO_ROOT_PASSWORD=sua_senha_segura_minio
# ... (demais variaveis)
AtenÃ§Ã£o: Mantenha o arquivo .env seguro e nÃ£o o adicione ao controle de versÃ£o pÃºblico. Ele jÃ¡ estÃ¡ incluÃ­do no .gitignore para sua seguranÃ§a.Passo 3: AdaptaÃ§Ã£o do Projeto (Portabilidade)Este script ajusta caminhos de arquivo internos para garantir que o projeto funcione em qualquer sistema operacional (Linux, macOS, Windows).# Execute o script de configuraÃ§Ã£o (CRUCIAL para portabilidade)
python configure.py

# Este script automatiza a adaptaÃ§Ã£o de caminhos para seu ambiente local,
# garantindo que o projeto funcione corretamente independentemente do seu sistema operacional.
# Ele tambÃ©m cria um backup automÃ¡tico antes de fazer as modificaÃ§Ãµes e pode realizar rollback em caso de falha.
Passo 4: InstalaÃ§Ã£o de DependÃªncias# Crie um ambiente virtual (recomendado para isolar as dependÃªncias do projeto)
python -m venv venv

# Ative o ambiente virtual
# Para Linux/macOS:
source venv/bin/activate
# Para Windows PowerShell:
# venv\Scripts\activate

# Instale todas as dependÃªncias Python listadas no requirements.txt
pip install -r requirements.txt

# Verifique se as principais bibliotecas foram instaladas
pip list | grep -E "(airflow|pyspark|pandas|great-expectations|streamlit|minio|psycopg2|sqlalchemy|cryptography)"
Passo 5: InicializaÃ§Ã£o da Infraestrutura DockerEste passo levanta todos os serviÃ§os essenciais (PostgreSQL, MinIO, Redis, Airflow Webserver, Airflow Scheduler).# O comando a seguir faz uma limpeza profunda e reconstrÃ³i as imagens para garantir um ambiente limpo.
# Isso Ã© crucial para que todas as configuraÃ§Ãµes e dependÃªncias (JARs do Spark) sejam aplicadas.
docker-compose down -v --rmi all
docker system prune -a --volumes -f
docker-compose up -d --build

# Verifique se os serviÃ§os Docker estÃ£o rodando e saudÃ¡veis
docker-compose ps

# Esperado (Status 'healthy' ou 'running'):
# - minio_object_storage (porta 9000/9001)
# - postgres_data_warehouse (porta 5432)
# - redis_cache_layer (porta 6379)
# - airflow_webserver (porta 8080)
# - airflow_scheduler
Passo 6: ConfiguraÃ§Ã£o do Security VaultEste passo popula o Vault de SeguranÃ§a com as credenciais para MinIO, PostgreSQL e APIs externas, utilizando a SECURITY_VAULT_SECRET_KEY do seu .env.# Acesse o shell do contÃªiner do scheduler (onde o Python e os plugins estÃ£o disponÃ­veis)
docker-compose exec airflow-scheduler bash

# Dentro do contÃªiner, exporte a SECURITY_VAULT_SECRET_KEY do seu .env
# Isso Ã© necessÃ¡rio para que o script setup_vault_secrets.py possa usÃ¡-la.
export SECURITY_VAULT_SECRET_KEY=$(grep 'SECURITY_VAULT_SECRET_KEY=' /opt/airflow/.env | cut -d '=' -f2)

# Popule o vault com as credenciais (MinIO, PostgreSQL, OpenWeatherMap, Masking Key)
# Este script lÃª as variÃ¡veis de ambiente e as armazena criptografadas no vault.json.
python /opt/airflow/scripts/setup_vault_secrets.py

# Saia do shell do contÃªiner
exit

# Opcional: Teste a conectividade dos serviÃ§os (requer que o Vault tenha sido populado)
python scripts/health_check.py

# SaÃ­da esperada:
# âœ… PostgreSQL: Connected
# âœ… MinIO: Connected  
# âœ… Redis: Connected
# âœ… Security Vault: Initialized (se o test_connections.py verificar isso)
Passo 7: InicializaÃ§Ã£o do AirflowEste passo inicia os componentes internos do Airflow (banco de dados de metadados, usuÃ¡rio admin).# Configure as variÃ¡veis de ambiente (se nÃ£o estiver usando um entrypoint que jÃ¡ faz isso)
export AIRFLOW_HOME=$(pwd)
export AIRFLOW__CORE__DAGS_FOLDER=${AIRFLOW_HOME}/dags
export AIRFLOW__CORE__PLUGINS_FOLDER=${AIRFLOW_HOME}/plugins
export AIRFLOW__CORE__LOAD_EXAMPLES=False

# Inicialize/atualize o banco de dados de metadados do Airflow
airflow db upgrade

# Crie o usuÃ¡rio administrador padrÃ£o para a UI do Airflow (admin/admin)
# O `|| true` evita que o comando falhe se o usuÃ¡rio jÃ¡ existir.
airflow users create \
    --username admin \
    --password admin \
    --firstname Admin \
    --lastname User \
    --role Admin \
    --email admin@example.com || true

# Os serviÃ§os `airflow-webserver` e `airflow-scheduler` jÃ¡ estÃ£o rodando
# (iniciados no Passo 5 via docker-compose up -d)
# NÃ£o Ã© necessÃ¡rio rodar `airflow scheduler` ou `airflow webserver` aqui novamente.
Passo 8: ExecuÃ§Ã£o do Dashboard# Abra um NOVO terminal ou aba de terminal e ative o ambiente virtual (se nao ativado)
# source venv/bin/activate # Linux/macOS
# venv\Scripts\activate     # Windows

# Execute o dashboard Streamlit
streamlit run dashboard/app.py --server.port 8501

# Acesse o dashboard no seu navegador: http://localhost:8501

# Opcional: Execute o Grafana (se configurado no docker-compose.yml e descomentado)
# Acesse Grafana no seu navegador: http://localhost:3000 (admin/admin)
ğŸ” VerificaÃ§Ã£o da InstalaÃ§Ã£oApÃ³s seguir os passos, verifique a saÃºde do ambiente:URLs de AcessoAirflow UI: http://localhost:8080 (UsuÃ¡rio: admin, Senha: admin)Streamlit Dashboard: http://localhost:8501MinIO Console: http://localhost:9001 (UsuÃ¡rio: minioadmin, Senha: minio_secure_2024 - ou a senha que vocÃª configurou no .env)Grafana: http://localhost:3000 (UsuÃ¡rio: admin, Senha: admin - se configurado)Testes de ConectividadeExecute no terminal na raiz do projeto (com o ambiente virtual ativado):# Teste todas as conexÃµes
python scripts/health_check.py

# SaÃ­da esperada:
# âœ… PostgreSQL: Connected
# âœ… MinIO: Connected  
# âœ… Redis: Connected
# âœ… Security Vault: Initialized (se o test_connections.py verificar isso)
ExecuÃ§Ã£o do PipelineApÃ³s verificar as URLs e conectividade, vocÃª pode ativar e monitorar suas DAGs na interface do Airflow.Acesse a Airflow UI (http://localhost:8080).Ative as DAGs clicando no botÃ£o "toggle" (interruptor) ao lado do nome de cada DAG.Dispare as DAGs clicando no Ã­cone de "Play" ao lado do nome da DAG para iniciar uma execuÃ§Ã£o manual.SugestÃ£o de Ordem de ExecuÃ§Ã£o (para gerar dados para as prÃ³ximas etapas):dag_coleta_segura_v1dag_03_consolidacao_e_mascaramento_v1dag_04_processamento_spark_seguro_v1dag_05_validacao_segura_v1dag_06_carrega_star_schema_segura_enterprise_v1dag_upload_bronze_minio_enterprise_v1 (se os dados locais de Olist nÃ£o estiverem no MinIO ainda)dag_upload_silver_minio_enterprise_v1dag_gerenciamento_lifecycle_enterprise_v1Monitore a execuÃ§Ã£o pela interface do Airflow (visualizaÃ§Ãµes Graph, Gantt, Logs). VocÃª tambÃ©m pode usar a CLI (no terminal com ambiente virtual ativado):# Monitore o status de uma DAG (exemplo)
airflow dags state dag_04_processamento_spark_seguro_v1
ğŸ› SoluÃ§Ã£o de ProblemasProblemas ComunsProblemaSoluÃ§Ã£oPorta jÃ¡ em usoIdentifique o processo usando a porta (netstat -tlnp | grep :<PORTA>) e finalize-o, ou altere a porta no docker-compose.yml e airflow.cfg.Erro de permissÃ£o DockerAdicione seu usuÃ¡rio ao grupo docker (sudo usermod -aG docker $USER) e reinicie o terminal ou a sessÃ£o (logout/login). No Windows/macOS, verifique se o Docker Desktop estÃ¡ rodando.spark-submit: command not foundVerifique o Dockerfile e airflow.cfg para a configuraÃ§Ã£o correta do PATH. A soluÃ§Ã£o final implementada no projeto usa export PATH="/home/airflow/.local/bin:${PATH}" no bash_command da DAG para garantir a detecÃ§Ã£o do spark-submit dentro do contÃªiner Airflow.Airflow nÃ£o inicia / DAGs 'Broken'Verifique os logs do contÃªiner airflow-webserver ou airflow-scheduler (docker-compose logs airflow-webserver). As mensagens de erro indicarÃ£o o problema (ex: SyntaxError em DAGs, problemas de conexÃ£o com DB/Redis, variÃ¡veis de ambiente ausentes).Credenciais nÃ£o encontradas no VaultCertifique-se de que o Passo 6: ConfiguraÃ§Ã£o do Security Vault foi executado corretamente, e que a SECURITY_VAULT_SECRET_KEY no seu .env Ã© a mesma usada para criptografar o vault.json.Logs e Debugging# Logs de um serviÃ§o Docker Compose (exemplo para o scheduler)
docker-compose logs -f airflow-scheduler

# Logs do Docker Compose de todos os serviÃ§os
docker-compose logs -f

# Logs de tarefas do Airflow (acessÃ­veis via UI do Airflow ou diretamente no host)
tail -f logs/dag_id=<NOME_DA_DAG>/<run_id>/<task_id>/*.log

# Teste individual de componentes (no terminal com ambiente virtual ativado)
python scripts/health_check.py
# Para verificar acesso ao Vault:
# python -c "from plugins.security_system.vault_manager_helper import VaultManager; import os; vm = VaultManager(vault_path=os.getenv('AIRFLOW_HOME','/opt/airflow')+'/plugins/security_system/vault.json', secret_key=os.getenv('SECURITY_VAULT_SECRET_KEY')); print(vm.get_secret('minio_local_credentials'))"
ğŸ“¦ Estrutura do Projetopipeline-dados-seguro/
â”œâ”€â”€ ğŸ“ dags/                  # DAGs do Apache Airflow (.py)
â”‚   â”œâ”€â”€ dag_01_coleta_segura_v1.py
â”‚   â”œâ”€â”€ dag_03_consolidacao_e_mascaramento_v1.py
â”‚   â”œâ”€â”€ dag_04_processamento_spark_seguro_v1.py
â”‚   â”œâ”€â”€ dag_05_validacao_segura_v1.py
â”‚   â”œâ”€â”€ dag_06_carrega_star_schema_segura_enterprise_v1.py
â”‚   â”œâ”€â”€ dag_coleta_dados_externos_enterprise_v1.py
â”‚   â”œâ”€â”€ dag_consolida_olist_enterprise_v1.py
â”‚   â”œâ”€â”€ dag_gerenciamento_lifecycle_enterprise_v1.py
â”‚   â”œâ”€â”€ dag_minio_para_postgresql_enterprise_v1.py
â”‚   â”œâ”€â”€ dag_upload_bronze_minio_enterprise_v1.py
â”‚   â””â”€â”€ dag_upload_silver_minio_enterprise_v1.py
â”œâ”€â”€ ğŸ“ plugins/               # Plugins customizados do Airflow
â”‚   â””â”€â”€ security_system/     # Framework de seguranÃ§a customizado
â”‚       â”œâ”€â”€ audit.py         # Audit Logger
â”‚       â”œâ”€â”€ connections.py   # Secure Connection Pool
â”‚       â”œâ”€â”€ data_protection.py # Mascaramento de Dados
â”‚       â”œâ”€â”€ exceptions.py    # ExceÃ§Ãµes customizadas
â”‚       â”œâ”€â”€ key_rotation.py  # RotaÃ§Ã£o de Chaves
â”‚       â”œâ”€â”€ monitoring.py    # Monitoramento de SeguranÃ§a
â”‚       â”œâ”€â”€ vault.py         # Airflow Security Manager (FAB override)
â”‚       â”œâ”€â”€ vault_manager_helper.py # Vault Manager (lÃ³gica de segredos)
â”‚       â””â”€â”€ verify_minio_upload.py # UtilitÃ¡rio de VerificaÃ§Ã£o MinIO
â”œâ”€â”€ ğŸ“ scripts/               # Scripts utilitÃ¡rios e de configuraÃ§Ã£o
â”‚   â”œâ”€â”€ configure.py         # Script de configuraÃ§Ã£o automÃ¡tica de paths
â”‚   â”œâ”€â”€ setup_vault_secrets.py # Script de setup de segredos no Vault
â”‚   â”œâ”€â”€ health_check.py      # Script de teste de conectividade
â”‚   â””â”€â”€ examples/            # Scripts Spark/Pandas/GE de exemplo
â”‚       â”œâ”€â”€ 01-coleta_ipca.py
â”‚       â”œâ”€â”€ 02-coleta_clima.py
â”‚       â”œâ”€â”€ 07-validacao_olist.py
â”‚       â”œâ”€â”€ 09-escrever_avro.py
â”‚       â”œâ”€â”€ 10-ler_avro.py
â”‚       â”œâ”€â”€ 12-processa_vendas.py # Job PySpark
â”‚       â”œâ”€â”€ 15-processador_stream_vendas.py # Processador de Stream
â”‚       â”œâ”€â”€ 18-popular_star_schema.py # PopulaÃ§Ã£o Star Schema
â”‚       â”œâ”€â”€ 19-validacao_great_expectations_avancada.py
â”‚       â”œâ”€â”€ 21-valida_vendas.py # ValidaÃ§Ã£o no Data Lake
â”‚       â””â”€â”€ 23-upload_criptografado_sse.py # Upload SSE
â”œâ”€â”€ ğŸ“ dashboard/             # Dashboard Streamlit
â”‚   â””â”€â”€ app.py
â”œâ”€â”€ ğŸ“ data/                  # Dados de entrada e saÃ­da (mapeado como volume)
â”‚   â”œâ”€â”€ olist/               # Datasets Olist e dados consolidados
â”‚   â””â”€â”€ security_vault.db    # Banco de dados SQLite do Vault (criptografado)
â”œâ”€â”€ ğŸ“ logs/                  # Logs do Airflow e de Auditoria (mapeado como volume)
â”œâ”€â”€ ğŸ“ docs/                  # DocumentaÃ§Ã£o adicional e imagens
â”‚   â””â”€â”€ images/              # Imagens para o README e apresentacao
â”œâ”€â”€ ğŸ“ init-scripts/          # Scripts de inicializacao de containers
â”‚   â””â”€â”€ entrypoint.sh        # Script de entrada customizado para Docker
â”œâ”€â”€ ğŸ³ docker-compose.yml     # Infraestrutura
â”œâ”€â”€ ğŸ“‹ requirements.txt      # Dependencias Python do projeto
â”œâ”€â”€ âš™ï¸ .env.example          # Template de variaveis de ambiente
â””â”€â”€ ğŸ“œ LICENSE               # Licenca do projeto (ex: MIT)
âœ… Checklist de ValidaÃ§Ã£o para a BancaEste checklist pode ser usado pela banca para validar a soluÃ§Ã£o:[ ] O repositÃ³rio Git Ã© pÃºblico e acessÃ­vel.[ ] O arquivo README.md estÃ¡ presente e bem formatado.[ ] Todos os prÃ©-requisitos de software e hardware estÃ£o claros.[ ] O script configure.py foi executado para adaptar os caminhos.[ ] Os serviÃ§os Docker (minio, postgres, redis, airflow-webserver, airflow-scheduler) estÃ£o rodando e saudÃ¡veis (docker-compose ps).[ ] O Security Vault foi populado com sucesso (python scripts/setup_vault_secrets.py executado).[ ] A Airflow UI estÃ¡ acessÃ­vel em http://localhost:8080.[ ] As DAGs aparecem na interface do Airflow (nÃ£o como 'Broken DAG').[ ] Pelo menos uma DAG (ex: dag_04_processamento_spark_seguro_v1) foi executada com sucesso.[ ] Os dados processados aparecem no MinIO (verificar em http://localhost:9001).[ ] O Dashboard Streamlit estÃ¡ funcionando (http://localhost:8501).[ ] Os logs de auditoria (logs/security_audit/) estÃ£o sendo gerados.VI. ğŸ“Š Resultados e EvidÃªnciasEste projeto nÃ£o Ã© apenas um conjunto de scripts; Ã© um ecossistema de dados funcional e seguro, validado por evidÃªncias concretas.ğŸ—„ï¸ Arquitetura de Data Lake em AÃ§Ã£o (MinIO)Nossa implementaÃ§Ã£o do Data Lake em MinIO segue a arquitetura Medallion, garantindo organizaÃ§Ã£o, qualidade e governanÃ§a dos dados em diferentes estÃ¡gios de processamento.VisÃ£o Geral dos Buckets: Os buckets b-prd.sand-ux-indc-brasil (Bronze), s-prd.sand-ux-indc-brasil (Silver), g-prd.sand-ux-indcs (Gold) e glacier-mock (Cold Storage) demonstram a separaÃ§Ã£o de camadas.ConteÃºdo da Camada Bronze: A camada Bronze armazena os dados brutos e imutÃ¡veis, como os datasets da Olist.ConteÃºdo da Camada Silver: A camada Silver contÃ©m os dados jÃ¡ limpos, consolidados e com PII mascarado, prontos para anÃ¡lises ou promoÃ§Ã£o.ğŸ“Š Qualidade de Dados Garantida (Great Expectations)A implementaÃ§Ã£o de Quality Gates com Great Expectations assegura que apenas dados de alta qualidade progridam no pipeline, prevenindo problemas a jusante.Log de Sucesso da ValidaÃ§Ã£o: A DAG de validaÃ§Ã£o com Great Expectations (dag_05_validacao_segura_v1) mostra o sucesso na aplicaÃ§Ã£o das expectativas de qualidade.âš¡ Pipeline em ExecuÃ§Ã£o e MÃ©tricas de Performance (Airflow UI)As DAGs no Airflow demonstram a orquestraÃ§Ã£o robusta, o paralelismo e a eficiÃªncia do pipeline.Grafo da DAG de MinIO para PostgreSQL: Demonstra a orquestraÃ§Ã£o do processo de ETL, com todas as tarefas concluÃ­das com sucesso.DuraÃ§Ã£o da Tarefa de Processamento Spark: EvidÃªncia da execuÃ§Ã£o do job Spark, fundamental para as transformaÃ§Ãµes em larga escala.DuraÃ§Ã£o da Tarefa de ConsolidaÃ§Ã£o e Mascaramento: Mostra a eficiÃªncia da etapa de processamento e proteÃ§Ã£o de dados.DuraÃ§Ã£o da Tarefa de Carga Star Schema: Ilustra o tempo necessÃ¡rio para popular o Data Mart.DuraÃ§Ã£o das Tarefas de Coleta e ValidaÃ§Ã£o (IPCA/Clima): Demonstra a execuÃ§Ã£o de mÃºltiplos pipelines de ingestÃ£o e qualidade.Logs do Gerenciamento de Lifecycle: Confirma que a lÃ³gica de movimentaÃ§Ã£o de dados (Hot para Cold Storage) estÃ¡ operando.ğŸ” SeguranÃ§a Implementada (Framework Customizado)O framework de seguranÃ§a Ã© um dos maiores destaques, garantindo que o pipeline Ã© robusto e estÃ¡ em conformidade.Credenciais Criptografadas: As credenciais para todos os serviÃ§os (MinIO, PostgreSQL, APIs externas) sÃ£o armazenadas criptografadas no Security Vault (vault.json) e recuperadas em tempo de execuÃ§Ã£o, garantindo que nunca estejam expostas no cÃ³digo ou em logs.Auditoria Completa: Cada operaÃ§Ã£o de seguranÃ§a e acesso a dados Ã© registrada detalhadamente pelo Audit Logger, fornecendo um rastro para conformidade (LGPD, SOX) e investigaÃ§Ã£o de incidentes.Mascaramento de PII: Demonstra o uso de tÃ©cnicas como hash e mascaramento estÃ¡tico para proteger informaÃ§Ãµes pessoalmente identificÃ¡veis.Criptografia Server-Side (SSE): Capacidade de fazer upload de arquivos para o MinIO com criptografia SSE-S3, protegendo os dados em repouso.IV. ğŸ§  Melhorias e ConsideraÃ§Ãµes FinaisDecisÃµes de Projeto e PrÃ¡ticas de ProduÃ§Ã£oÃ‰ fundamental que as decisÃµes de projeto reflitam a consciÃªncia sobre as diferenÃ§as entre um ambiente de demonstraÃ§Ã£o/desenvolvimento e um sistema produtivo em larga escala.Modularidade e Reuso de Componentes de SeguranÃ§a: O framework de seguranÃ§a (security_system nos plugins) foi projetado para ser um conjunto de mÃ³dulos reutilizÃ¡veis.VaultManager vs. AirflowSecurityManager: Mantivemos a AirflowSecurityManager (plugins/security_system/vault.py) focada em estender a seguranÃ§a da UI do Airflow. A lÃ³gica de gestÃ£o real de segredos (criptografia, leitura/escrita no vault.json) foi delegada ao VaultManager (plugins/security_system/vault_manager_helper.py). Essa separaÃ§Ã£o Ã© crucial para clareza e manutenÃ§Ã£o em um ambiente enterprise.Reuso: Em vez de replicar a lÃ³gica de os.getenv('SECRET_KEY') e instanciaÃ§Ã£o do Vault em cada DAG/script, o VaultManager encapsula isso, e as DAGs/scripts apenas o instanciam e o utilizam.ConfiguraÃ§Ã£o de Credenciais:No Case (Demo): Para fins de reprodutibilidade e simplicidade da demonstraÃ§Ã£o, as credenciais para serviÃ§os como MinIO e PostgreSQL sÃ£o lidas de variÃ¡veis de ambiente do .env (populadas via docker-compose.yml) e entÃ£o inseridas no Security Vault via scripts/setup_vault_secrets.py.Em ProduÃ§Ã£o: Em um ambiente real, a SECURITY_VAULT_SECRET_KEY viria de um serviÃ§o de gerenciamento de segredos da nuvem (ex: AWS Secrets Manager, HashiCorp Vault Server) ou de um sistema de orquestraÃ§Ã£o de segredos, nunca de um arquivo .env versionado ou acessÃ­vel diretamente. O setup_vault_secrets.py seria parte de um processo de deploy seguro.AutomaÃ§Ã£o da RefatoraÃ§Ã£o (refinar_projeto.py):No Case: Um script Python foi desenvolvido para automatizar a adaptaÃ§Ã£o de caminhos hardcoded e a inserÃ§Ã£o de blocos de validaÃ§Ã£o de seguranÃ§a em outros arquivos Python.Valor Demonstrado: Isso mostra uma mentalidade de engenharia que busca resolver problemas de forma programÃ¡tica, aumenta a produtividade e reduz erros, uma prÃ¡tica essencial em equipes de alta performance.ğŸš€ Melhorias Propostas para PrÃ³ximas IteraÃ§ÃµesPara evoluir este projeto para um nÃ­vel de produÃ§Ã£o ainda mais avanÃ§ado e escalÃ¡vel, as seguintes melhorias sÃ£o propostas:Infraestrutura como CÃ³digo (IaC)Terraform: Para automatizaÃ§Ã£o completa da provisÃ£o de recursos em nuvem (ex: EC2, RDS, S3, EMR, MWAA).Ansible: Para configuraÃ§Ã£o e automaÃ§Ã£o de deploy de aplicaÃ§Ãµes nos servidores.CI/CD para PipelinesGitHub Actions / GitLab CI: Para automaÃ§Ã£o de testes unitÃ¡rios, de integraÃ§Ã£o e end-to-end.Deploy Automatizado: Novas versÃµes de DAGs e cÃ³digos seriam implantadas automaticamente em ambientes de teste e produÃ§Ã£o.Testes de IntegraÃ§Ã£o Automatizados: CenÃ¡rios de teste que validam o fluxo de dados entre mÃºltiplos componentes.CatÃ¡logo de DadosIntegraÃ§Ã£o com Apache Atlas ou Amundsen: Para documentaÃ§Ã£o automÃ¡tica de metadados, linhagem de dados (data lineage) e descoberta de dados.Observabilidade AvanÃ§adaMÃ©tricas Customizadas com Prometheus / Grafana: Para monitoramento granular de performance do pipeline e recursos da infraestrutura.Alertas Proativos: ConfiguraÃ§Ã£o de alertas em caso de anomalias ou falhas crÃ­ticas.Distributed Tracing com Jaeger / OpenTelemetry: Para depuraÃ§Ã£o de pipelines complexos em ambientes distribuÃ­dos.ğŸ“ˆ Escalabilidade e Performance (ProjeÃ§Ãµes)AspectoImplementaÃ§Ã£o Atual (Local Docker)Melhoria Proposta (Cloud / OtimizaÃ§Ãµes)Volume~100k registros OlistPetabytes (particionamento horizontal, sharding)LatÃªncia< 30 segundos (pipeline end-to-end)< 10 segundos (para ingestÃ£o, com Kafka/Redis)ConcorrÃªncia3 DAGs paralelas (com CeleryExecutor)10+ DAGs e tasks simultÃ¢neas (Kubernetes, Fargate)MonitoramentoLogs bÃ¡sicos, Airflow UIAPM completo, dashboards Grafana, alertas SMS/emailPersistÃªnciaVolumes Docker, MinIO localS3/GCS/Azure Blob Storage, Databases gerenciadosğŸ† ConsideraÃ§Ãµes FinaisEste case entrega uma soluÃ§Ã£o de dados enterprise-grade, segura, confiÃ¡vel, escalÃ¡vel e totalmente reprodutÃ­vel. As decisÃµes de projeto, como a criaÃ§Ã£o de um framework de seguranÃ§a customizado e a automaÃ§Ã£o de tarefas de desenvolvimento/deploy, demonstram um domÃ­nio de conceitos que vÃ£o muito alÃ©m do bÃ¡sico, focando nos desafios reais de um ambiente corporativo moderno.A arquitetura implementada Ã© production-ready em seus princÃ­pios e pode ser facilmente adaptada e estendida para ambientes de nuvem em grande escala, mantendo os mesmos pilares de seguranÃ§a, qualidade e governanÃ§a.V. ğŸ› ï¸ Reprodutibilidade da ArquiteturaEste projeto foi construÃ­do com foco na reprodutibilidade e portabilidade, utilizando Docker para isolar o ambiente. As instruÃ§Ãµes a seguir detalham como configurar e executar o projeto em qualquer mÃ¡quina (macOS, Linux, Windows) com Docker instalado.PrÃ©-requisitos do SistemaSoftwares NecessÃ¡riosPython 3.8+ (com pip)Git (versÃ£o 2.25+)Docker Desktop (para Windows/macOS) ou Docker Engine (para Linux)Docker Compose (geralmente incluÃ­do com o Docker Desktop/Engine)Apache Spark 3.5+: Embora as DAGs rodem o Spark via spark-submit dentro do contÃªiner Airflow, ter o Spark instalado localmente pode ser Ãºtil para depuraÃ§Ã£o ou execuÃ§Ã£o de scripts Spark standalone.macOS: brew install apache-sparkLinux/Windows: Download oficialRecursos de Hardware MÃ­nimosRAM: 8GB (recomendado 16GB para performance ideal com Spark e Airflow)Armazenamento: 10GB livresCPU: 4 cores (recomendado 8 cores)ğŸš€ InstalaÃ§Ã£o e ExecuÃ§Ã£oSiga os passos rigorosamente para garantir a correta inicializaÃ§Ã£o do ambiente.Passo 1: Clonagem do RepositÃ³rioAbra seu terminal e execute:# Clone o repositÃ³rio
git clone https://github.com/felipesbonatti/case-data-master-engenharia-de-dados.git
cd case-data-master-engenharia-de-dados

# Verifique a estrutura do projeto
# (O comando 'tree' pode nÃ£o estar disponÃ­vel no Windows por padrÃ£o, use 'dir /s /b' ou explore manualmente)
tree -L 2
Passo 2: ConfiguraÃ§Ã£o do AmbienteEste passo Ã© CRÃTICO para a seguranÃ§a do Vault e o funcionamento do pipeline.# Crie o arquivo de ambiente (.env) a partir do template
cp .env.example .env

# Gere uma chave de criptografia segura para o Vault e adicione ao .env
# Esta chave Ã© fundamental para criptografar/decriptar seus segredos no Vault.
python -c "from cryptography.fernet import Fernet; print('SECURITY_VAULT_SECRET_KEY=' + Fernet.generate_key().decode())" >> .env

# Abra o arquivo .env e configure suas API keys reais e senhas fortes.
# EXEMPLO DE .env (SUBSTITUA OS VALORES PADRÃƒO POR SEUS VALORES REAIS E SEGUROS):
# SECURITY_VAULT_SECRET_KEY=SUA_CHAVE_FERNET_GERADA_AQUI
# OPENWEATHER_API_KEY=SUA_CHAVE_DA_OPENWEATHERMAP_AQUI
# POSTGRES_USER=airflow_user_real
# POSTGRES_PASSWORD=sua_senha_segura_postgres
# MINIO_ROOT_USER=minio_admin_real
# MINIO_ROOT_PASSWORD=sua_senha_segura_minio
# ... (demais variaveis)
AtenÃ§Ã£o: Mantenha o arquivo .env seguro e nÃ£o o adicione ao controle de versÃ£o pÃºblico. Ele jÃ¡ estÃ¡ incluÃ­do no .gitignore para sua seguranÃ§a.Passo 3: AdaptaÃ§Ã£o do Projeto (Portabilidade)Este script ajusta caminhos de arquivo internos para garantir que o projeto funcione em qualquer sistema operacional (Linux, macOS, Windows).# Execute o script de configuraÃ§Ã£o (CRUCIAL para portabilidade)
python configure.py

# Este script automatiza a adaptaÃ§Ã£o de caminhos para seu ambiente local,
# garantindo que o projeto funcione corretamente independentemente do seu sistema operacional.
# Ele tambÃ©m cria um backup automÃ¡tico antes de fazer as modificaÃ§Ãµes e pode realizar rollback em caso de falha.
Passo 4: InstalaÃ§Ã£o de DependÃªncias# Crie um ambiente virtual (recomendado para isolar as dependÃªncias do projeto)
python -m venv venv

# Ative o ambiente virtual
# Para Linux/macOS:
source venv/bin/activate
# Para Windows PowerShell:
# venv\Scripts\activate

# Instale todas as dependÃªncias Python listadas no requirements.txt
pip install -r requirements.txt

# Verifique se as principais bibliotecas foram instaladas
pip list | grep -E "(airflow|pyspark|pandas|great-expectations|streamlit|minio|psycopg2|sqlalchemy|cryptography)"
Passo 5: InicializaÃ§Ã£o da Infraestrutura DockerEste passo levanta todos os serviÃ§os essenciais (PostgreSQL, MinIO, Redis, Airflow Webserver, Airflow Scheduler).# O comando a seguir faz uma limpeza profunda e reconstrÃ³i as imagens para garantir um ambiente limpo.
# Isso Ã© crucial para que todas as configuraÃ§Ãµes e dependÃªncias (JARs do Spark) sejam aplicadas.
docker-compose down -v --rmi all
docker system prune -a --volumes -f
docker-compose up -d --build

# Verifique se os serviÃ§os Docker estÃ£o rodando e saudÃ¡veis
docker-compose ps

# Esperado (Status 'healthy' ou 'running'):
# - minio_object_storage (porta 9000/9001)
# - postgres_data_warehouse (porta 5432)
# - redis_cache_layer (porta 6379)
# - airflow_webserver (porta 8080)
# - airflow_scheduler
Passo 6: ConfiguraÃ§Ã£o do Security VaultEste passo popula o Vault de SeguranÃ§a com as credenciais para MinIO, PostgreSQL e APIs externas, utilizando a SECURITY_VAULT_SECRET_KEY do seu .env.# Acesse o shell do contÃªiner do scheduler (onde o Python e os plugins estÃ£o disponÃ­veis)
docker-compose exec airflow-scheduler bash

# Dentro do contÃªiner, exporte a SECURITY_VAULT_SECRET_KEY do seu .env
# Isso Ã© necessÃ¡rio para que o script setup_vault_secrets.py possa usÃ¡-la.
export SECURITY_VAULT_SECRET_KEY=$(grep 'SECURITY_VAULT_SECRET_KEY=' /opt/airflow/.env | cut -d '=' -f2)

# Popule o vault com as credenciais (MinIO, PostgreSQL, OpenWeatherMap, Masking Key)
# Este script lÃª as variÃ¡veis de ambiente e as armazena criptografadas no vault.json.
python /opt/airflow/scripts/setup_vault_secrets.py

# Saia do shell do contÃªiner
exit

# Opcional: Teste a conectividade dos serviÃ§os (requer que o Vault tenha sido populado)
python scripts/health_check.py

# SaÃ­da esperada:
# âœ… PostgreSQL: Connected
# âœ… MinIO: Connected  
# âœ… Redis: Connected
# âœ… Security Vault: Initialized (se o test_connections.py verificar isso)
Passo 7: InicializaÃ§Ã£o do AirflowEste passo inicia os componentes internos do Airflow (banco de dados de metadados, usuÃ¡rio admin).# Configure as variÃ¡veis de ambiente (se nÃ£o estiver usando um entrypoint que jÃ¡ faz isso)
export AIRFLOW_HOME=$(pwd)
export AIRFLOW__CORE__DAGS_FOLDER=${AIRFLOW_HOME}/dags
export AIRFLOW__CORE__PLUGINS_FOLDER=${AIRFLOW_HOME}/plugins
export AIRFLOW__CORE__LOAD_EXAMPLES=False

# Inicialize/atualize o banco de dados de metadados do Airflow
airflow db upgrade

# Crie o usuÃ¡rio administrador padrÃ£o para a UI do Airflow (admin/admin)
# O `|| true` evita que o comando falhe se o usuÃ¡rio jÃ¡ existir.
airflow users create \
    --username admin \
    --password admin \
    --firstname Admin \
    --lastname User \
    --role Admin \
    --email admin@example.com || true

# Os serviÃ§os `airflow-webserver` e `airflow-scheduler` jÃ¡ estÃ£o rodando
# (iniciados no Passo 5 via docker-compose up -d)
# NÃ£o Ã© necessÃ¡rio rodar `airflow scheduler` ou `airflow webserver` aqui novamente.
Passo 8: ExecuÃ§Ã£o do Dashboard# Abra um NOVO terminal ou aba de terminal e ative o ambiente virtual (se nao ativado)
# source venv/bin/activate # Linux/macOS
# venv\Scripts\activate     # Windows

# Execute o dashboard Streamlit
streamlit run dashboard/app.py --server.port 8501

# Acesse o dashboard no seu navegador: http://localhost:8501

# Opcional: Execute o Grafana (se configurado no docker-compose.yml e descomentado)
# Acesse Grafana no seu navegador: http://localhost:3000 (admin/admin)
ğŸ” VerificaÃ§Ã£o da InstalaÃ§Ã£oApÃ³s seguir os passos, verifique a saÃºde do ambiente:URLs de AcessoAirflow UI: http://localhost:8080 (UsuÃ¡rio: admin, Senha: admin)Streamlit Dashboard: http://localhost:8501MinIO Console: http://localhost:9001 (UsuÃ¡rio: minioadmin, Senha: minio_secure_2024 - ou a senha que vocÃª configurou no .env)Grafana: http://localhost:3000 (UsuÃ¡rio: admin, Senha: admin - se configurado)Testes de ConectividadeExecute no terminal na raiz do projeto (com o ambiente virtual ativado):# Teste todas as conexÃµes
python scripts/health_check.py

# SaÃ­da esperada:
# âœ… PostgreSQL: Connected
# âœ… MinIO: Connected  
# âœ… Redis: Connected
# âœ… Security Vault: Initialized (se o test_connections.py verificar isso)
ExecuÃ§Ã£o do PipelineApÃ³s verificar as URLs e conectividade, vocÃª pode ativar e monitorar suas DAGs na interface do Airflow.Acesse a Airflow UI (http://localhost:8080).Ative as DAGs clicando no botÃ£o "toggle" (interruptor) ao lado do nome de cada DAG.Dispare as DAGs clicando no Ã­cone de "Play" ao lado do nome da DAG para iniciar uma execuÃ§Ã£o manual.SugestÃ£o de Ordem de ExecuÃ§Ã£o (para gerar dados para as prÃ³ximas etapas):dag_coleta_segura_v1dag_03_consolidacao_e_mascaramento_v1dag_04_processamento_spark_seguro_v1dag_05_validacao_segura_v1dag_06_carrega_star_schema_segura_enterprise_v1dag_upload_bronze_minio_enterprise_v1 (se os dados locais de Olist nÃ£o estiverem no MinIO ainda)dag_upload_silver_minio_enterprise_v1dag_gerenciamento_lifecycle_enterprise_v1Monitore a execuÃ§Ã£o pela interface do Airflow (visualizaÃ§Ãµes Graph, Gantt, Logs). VocÃª tambÃ©m pode usar a CLI (no terminal com ambiente virtual ativado):# Monitore o status de uma DAG (exemplo)
airflow dags state dag_04_processamento_spark_seguro_v1
ğŸ› SoluÃ§Ã£o de ProblemasProblemas ComunsProblemaSoluÃ§Ã£oPorta jÃ¡ em usoIdentifique o processo usando a porta (netstat -tlnp | grep :<PORTA>) e finalize-o, ou altere a porta no docker-compose.yml e airflow.cfg.Erro de permissÃ£o DockerAdicione seu usuÃ¡rio ao grupo docker (sudo usermod -aG docker $USER) e reinicie o terminal ou a sessÃ£o (logout/login). No Windows/macOS, verifique se o Docker Desktop estÃ¡ rodando.spark-submit: command not foundVerifique o Dockerfile e airflow.cfg para a configuraÃ§Ã£o correta do PATH. A soluÃ§Ã£o final implementada no projeto usa export PATH="/home/airflow/.local/bin:${PATH}" no bash_command da DAG para garantir a detecÃ§Ã£o do spark-submit dentro do contÃªiner Airflow.Airflow nÃ£o inicia / DAGs 'Broken'Verifique os logs do contÃªiner airflow-webserver ou airflow-scheduler (docker-compose logs airflow-webserver). As mensagens de erro indicarÃ£o o problema (ex: SyntaxError em DAGs, problemas de conexÃ£o com DB/Redis, variÃ¡veis de ambiente ausentes).Credenciais nÃ£o encontradas no VaultCertifique-se de que o Passo 6: ConfiguraÃ§Ã£o do Security Vault foi executado corretamente, e que a SECURITY_VAULT_SECRET_KEY no seu .env Ã© a mesma usada para criptografar o vault.json.Logs e Debugging# Logs de um serviÃ§o Docker Compose (exemplo para o scheduler)
docker-compose logs -f airflow-scheduler

# Logs do Docker Compose de todos os serviÃ§os
docker-compose logs -f

# Logs de tarefas do Airflow (acessÃ­veis via UI do Airflow ou diretamente no host)
tail -f logs/dag_id=<NOME_DA_DAG>/<run_id>/<task_id>/*.log

# Teste individual de componentes (no terminal com ambiente virtual ativado)
python scripts/health_check.py
# Para verificar acesso ao Vault:
# python -c "from plugins.security_system.vault_manager_helper import VaultManager; import os; vm = VaultManager(vault_path=os.getenv('AIRFLOW_HOME','/opt/airflow')+'/plugins/security_system/vault.json', secret_key=os.getenv('SECURITY_VAULT_SECRET_KEY')); print(vm.get_secret('minio_local_credentials'))"

# ğŸš€ Pipeline de Dados Seguro: Da IngestÃ£o Ã  VisualizaÃ§Ã£o AnalÃ­tica

<p align="center">
  <img src="docs/images/pipeline_cover.png" alt="Capa do Projeto" width="700"/>
</p>

<p align="center">
  <em>Uma arquitetura de engenharia de dados robusta, segura e 100% reprodutÃ­vel para ambientes empresariais.</em>
</p>

---

## ğŸ“‹ Ãndice

* [I. ğŸ¯ Objetivo do Case](#i--objetivo-do-case)
* [II. ğŸ›ï¸ Arquitetura da SoluÃ§Ã£o](#ii--arquitetura-da-soluÃ§Ã£o)
* [III. âš™ï¸ O Case em AÃ§Ã£o](#iii--o-case-em-aÃ§Ã£o)
* [IV. ğŸ§  Melhorias e PrÃ³ximos Passos](#iv--melhorias-e-prÃ³ximos-passos)
* [V. ğŸ› ï¸ Reprodutibilidade da Arquitetura](#v--reprodutibilidade-da-arquitetura)
* [VI. ğŸ“Š Resultados e EvidÃªncias](#vi--resultados-e-evidÃªncias)

---

## I. ğŸ¯ Objetivo do Case

### Desafio
Construir um **pipeline de dados ponta a ponta** em uma arquitetura **100% local e open-source**, garantindo total reprodutibilidade. A soluÃ§Ã£o abrange desde a ingestÃ£o de mÃºltiplas fontes atÃ© a criaÃ§Ã£o de um dashboard analÃ­tico interativo, com um foco rigoroso em **seguranÃ§a, qualidade, governanÃ§a e automaÃ§Ã£o**.

### CompetÃªncias Demonstradas
Este projeto Ã© uma evidÃªncia prÃ¡tica de **competÃªncias avanÃ§adas em Engenharia de Dados**, abrangendo:

- **ğŸ”§ OrquestraÃ§Ã£o de fluxos complexos e resilientes** com Apache Airflow.
- **âš¡ Processamento de dados em larga escala** com Apache Spark.
- **ğŸ—ï¸ Modelagem dimensional e arquitetura Star Schema** para Data Warehouses.
- **ğŸ” Desenvolvimento de um Framework de SeguranÃ§a customizado**, com Vault de segredos e auditoria.
- **ğŸ“Š ImplementaÃ§Ã£o de Quality Gates** com Great Expectations.
- **ğŸ›ï¸ ConstruÃ§Ã£o de uma arquitetura de Data Lake Medallion** (Bronze, Silver, Gold) com MinIO.
- **ğŸ¤– AutomaÃ§Ã£o de processos** de setup e refatoraÃ§Ã£o de cÃ³digo.
- **ğŸ“ˆ VisualizaÃ§Ã£o analÃ­tica e Business Intelligence** com Streamlit e Grafana.
- **ğŸ³ GestÃ£o de ambientes com Docker e Docker Compose** para portabilidade total.

### Valor de NegÃ³cio
A soluÃ§Ã£o demonstra uma abordagem de engenharia completa, preparada para os desafios de ambientes de produÃ§Ã£o. Ela possibilita o processamento seguro de dados heterogÃªneos, a geraÃ§Ã£o de insights acionÃ¡veis e serve como um **template robusto para pipelines futuros com foco em compliance e governanÃ§a**.

---

## II. ğŸ›ï¸ Arquitetura da SoluÃ§Ã£o

### VisÃ£o Geral da Arquitetura

> **Nota Importante:** O GitHub nÃ£o renderiza diagramas Mermaid diretamente. A melhor prÃ¡tica Ã© gerar uma imagem (PNG/SVG) a partir do cÃ³digo Mermaid e inseri-la aqui. VocÃª pode usar o [Editor Online do Mermaid](https://mermaid.live) para gerar a imagem.

<p align="center">
  <img src="docs/images/architecture_diagram.png" alt="VisÃ£o Geral da Arquitetura" width="800"/>
  <br>
  <em><strong>AÃ§Ã£o NecessÃ¡ria:</strong> Gere a imagem do seu diagrama Mermaid e salve em <code>docs/images/architecture_diagram.png</code> (ou atualize o caminho).</em>
</p>


### Detalhamento dos Componentes

- **Apache Airflow:** O cÃ©rebro da orquestraÃ§Ã£o, gerenciando o agendamento e a execuÃ§Ã£o de todos os pipelines (DAGs).
- **PostgreSQL:** Atua como o banco de metadados do Airflow e como o Data Warehouse final, hospedando o modelo Star Schema.
- **MinIO:** Simula um Object Storage (compatÃ­vel com S3) para nosso Data Lake, com as camadas Bronze, Silver, Gold e Cold.
- **Redis:** Broker de mensagens para o Celery Executor do Airflow, permitindo a execuÃ§Ã£o distribuÃ­da de tarefas.
- **Apache Spark:** Motor de processamento distribuÃ­do para transformaÃ§Ãµes e agregaÃ§Ãµes em larga escala.
- **Streamlit:** Interface para construir dashboards interativos de Business Intelligence.
- **Grafana:** Ferramenta para monitoramento e visualizaÃ§Ã£o de mÃ©tricas operacionais.

### ğŸ” Framework de SeguranÃ§a Customizado (`plugins/security_system/`)

> **Diferencial do Projeto:** Este framework garante a integridade, confidencialidade e rastreabilidade dos dados em todo o pipeline, demonstrando conhecimento avanÃ§ado em seguranÃ§a e engenharia de software.

- **Security Vault:** Um cofre digital criptografado (AES-128) que armazena credenciais sensÃ­veis e as fornece em tempo de execuÃ§Ã£o, eliminando o *hardcoding*.
- **Audit Logger:** Um sistema de auditoria que registra todas as operaÃ§Ãµes crÃ­ticas (acessos a segredos, transformaÃ§Ãµes, validaÃ§Ãµes), essencial para conformidade (LGPD, SOX).
- **Secure Connection Pool:** Um gerenciador que facilita a obtenÃ§Ã£o segura de conexÃµes a serviÃ§os externos (MinIO, PostgreSQL), abstraindo o uso do Vault.

### ğŸ—„ï¸ Data Lake com Arquitetura Medallion (MinIO)

| Camada         | DescriÃ§Ã£o                    | CaracterÃ­sticas                                              |
| :------------- | :--------------------------- | :----------------------------------------------------------- |
| ğŸ¥‰ **Bronze** | Dados Brutos e ImutÃ¡veis     | Raw data, schema-on-read, auditÃ¡vel.                         |
| ğŸ¥ˆ **Silver** | Dados Limpos e Padronizados  | LGPD compliant (PII mascarado), validado, pronto para anÃ¡lise. |
| ğŸ¥‡ **Gold** | Dados Agregados e Otimizados | Regras de negÃ³cio aplicadas, sumarizado, alta performance para BI. |
| ğŸ§Š **Cold Storage** | Dados Arquivados/Inativos    | OtimizaÃ§Ã£o de custos, retenÃ§Ã£o de longo prazo.               |

### âš¡ Processamento DistribuÃ­do (Apache Spark)
Utilizado para transformaÃ§Ãµes complexas, executando limpeza, normalizaÃ§Ã£o, enriquecimento e agregaÃ§Ã£o, com credenciais injetadas de forma segura em tempo de execuÃ§Ã£o e persistindo os dados no formato colunar otimizado Parquet.

### ğŸ“Š Qualidade de Dados (Great Expectations)
Implementamos **Quality Gates** em etapas crÃ­ticas. Os pipelines sÃ£o interrompidos automaticamente se as expectativas de qualidade nÃ£o forem atendidas, prevenindo a propagaÃ§Ã£o de dados ruins e garantindo a confianÃ§a nos resultados.

---

## III. âš™ï¸ O Case em AÃ§Ã£o

### Fluxo de Trabalho do Pipeline

<p align="center">
  <img src="docs/images/pipeline_flow.png" alt="Fluxo de Trabalho do Pipeline" width="800"/>
  <br>
  <em><strong>AÃ§Ã£o NecessÃ¡ria:</strong> Gere a imagem do seu segundo diagrama e salve em <code>docs/images/pipeline_flow.png</code> (ou atualize o caminho).</em>
</p>

### ğŸ”„ Etapas Detalhadas do Pipeline

1.  **Coleta Segura:**
    - **Objetivo:** IngestÃ£o de dados brutos de APIs e datasets.
    - **SeguranÃ§a:** Credenciais sÃ£o obtidas do Security Vault.
    - **Destino:** Camada **Bronze** do MinIO.

2.  **ConsolidaÃ§Ã£o e Mascaramento PII:**
    - **Objetivo:** Limpar, unificar e proteger dados sensÃ­veis (LGPD).
    - **Destino:** Camada **Silver** do MinIO.

3.  **Processamento em Larga Escala com Spark:**
    - **Objetivo:** Transformar dados da camada Silver em agregados para BI.
    - **SeguranÃ§a:** Credenciais para o MinIO sÃ£o injetadas de forma segura no Spark.
    - **Destino:** Camada **Gold** do MinIO.

4.  **ValidaÃ§Ã£o de Qualidade:**
    - **Objetivo:** Assegurar a integridade dos dados com Great Expectations, atuando como um "Quality Gate".
    - **Auditoria:** Resultados das validaÃ§Ãµes sÃ£o registrados no Audit Logger.

5.  **Carga no Data Mart (Star Schema):**
    - **Objetivo:** Carregar dados da camada Gold para o PostgreSQL de forma transacional (ACID).
    - **SeguranÃ§a:** ConexÃ£o segura ao PostgreSQL utilizando credenciais do Vault.

6.  **Gerenciamento de Lifecycle:**
    - **Objetivo:** Otimizar custos movendo dados antigos da camada Bronze para o Cold Storage.

### Fontes de Dados Integradas

| Fonte           | Tipo       | DescriÃ§Ã£o                        | Volume Simulado |
| :-------------- | :--------- | :------------------------------- | :-------------- |
| **Banco Central** | API REST   | Indicadores econÃ´micos (IPCA)    | Pequeno         |
| **OpenWeather** | API REST   | Dados meteorolÃ³gicos por regiÃ£o  | Pequeno         |
| **Olist** | Dataset CSV | Dados reais de e-commerce brasileiro | Grande          |

---

## IV. ğŸ§  Melhorias e PrÃ³ximos Passos

### ğŸš€ Melhorias Propostas
- **ğŸ—ï¸ Infraestrutura como CÃ³digo (IaC):** Utilizar **Terraform** e **Ansible**.
- **ğŸ”„ CI/CD para Pipelines:** Implementar **GitHub Actions / GitLab CI**.
- **ğŸ“š CatÃ¡logo de Dados:** Integrar com **Apache Atlas** ou **Amundsen**.
- **ğŸ”­ Observabilidade AvanÃ§ada:** Usar **Prometheus / Grafana** e **Jaeger / OpenTelemetry**.

### ğŸ“ˆ ProjeÃ§Ãµes de Escalabilidade

| Aspecto       | ImplementaÃ§Ã£o Atual (Local) | Proposta de Melhoria (Cloud)             |
| :------------ | :-------------------------- | :--------------------------------------- |
| **Volume** | ~100k registros             | **Petabytes** (particionamento, sharding) |
| **LatÃªncia** | < 30s (end-to-end)          | **< 10s** (ingestÃ£o com Kafka/Redis)      |
| **ConcorrÃªncia**| 3 DAGs paralelas            | **10+ DAGs e tasks simultÃ¢neas** (K8s)    |
| **Monitoramento** | Logs bÃ¡sicos, Airflow UI    | **APM completo**, dashboards, alertas    |

### ğŸ† ConsideraÃ§Ãµes Finais

> Este case entrega uma soluÃ§Ã£o de dados **enterprise-grade, segura, confiÃ¡vel e totalmente reprodutÃ­vel**. As decisÃµes de projeto demonstram um domÃ­nio de conceitos que vÃ£o muito alÃ©m do bÃ¡sico, focando nos desafios reais de um ambiente corporativo. A arquitetura estÃ¡ pronta para ser adaptada e estendida para ambientes de nuvem em grande escala.

---

## V. ğŸ› ï¸ Reprodutibilidade da Arquitetura

### PrÃ©-requisitos e InstalaÃ§Ã£o

1.  **PrÃ©-requisitos:**
    - Python 3.8+, Git, Docker e Docker Compose.
    - **Hardware MÃ­nimo:** 8GB RAM (16GB recomendado), 4 CPU cores, 10GB de armazenamento.

2.  **Clonagem do RepositÃ³rio:**
    ```bash
    git clone [https://github.com/felipesbonatti/case-data-master-engenharia-de-dados.git](https://github.com/felipesbonatti/case-data-master-engenharia-de-dados.git)
    cd case-data-master-engenharia-de-dados
    ```

3.  **ConfiguraÃ§Ã£o do Ambiente:**
    > **AtenÃ§Ã£o:** Este passo Ã© CRÃTICO para a seguranÃ§a e funcionamento do pipeline.

    ```bash
    # Crie o arquivo de ambiente a partir do template
    cp .env.example .env

    # Gere uma chave de criptografia segura e adicione-a ao .env
    python -c "from cryptography.fernet import Fernet; print('SECURITY_VAULT_SECRET_KEY=' + Fernet.generate_key().decode())" >> .env

    # ABRA O ARQUIVO .env E CONFIGURE SUAS API KEYS E SENHAS REAIS!
    ```

4.  **InicializaÃ§Ã£o da Infraestrutura:**
    ```bash
    # Garante um ambiente limpo, reconstruindo as imagens e dependÃªncias
    docker-compose down -v --rmi all && docker system prune -a --volumes -f

    # Inicia todos os serviÃ§os em background
    docker-compose up -d --build
    ```

5.  **ConfiguraÃ§Ã£o do Security Vault e Airflow:**
    ```bash
    # Acesse o contÃªiner do Airflow scheduler
    docker-compose exec airflow-scheduler bash

    # Dentro do contÃªiner, popule o Vault com as credenciais do seu .env
    python /opt/airflow/scripts/setup_vault_secrets.py
    
    # Saia do contÃªiner
    exit

    # Inicialize o banco de dados e crie o usuÃ¡rio admin
    docker-compose exec airflow-webserver airflow db upgrade
    docker-compose exec airflow-webserver airflow users create \
        --username admin --password admin \
        --firstname Admin --lastname User \
        --role Admin --email admin@example.com || true
    ```

### VerificaÃ§Ã£o e ExecuÃ§Ã£o
- **Airflow UI:** `http://localhost:8080` (admin/admin)
- **MinIO Console:** `http://localhost:9001` (usuÃ¡rio/senha do seu `.env`)
- **Streamlit Dashboard:** Execute com `streamlit run dashboard/app.py` e acesse `http://localhost:8501`.

---

## VI. ğŸ“Š Resultados e EvidÃªncias

### ğŸ—„ï¸ Data Lake em AÃ§Ã£o (MinIO)
A implementaÃ§Ã£o no MinIO segue a arquitetura Medallion, com buckets separados para as camadas **Bronze**, **Silver** e **Gold**, demonstrando organizaÃ§Ã£o e governanÃ§a.

### ğŸ“Š Qualidade de Dados Garantida (Great Expectations)
A DAG `dag_05_validacao_segura_v1` demonstra o sucesso na aplicaÃ§Ã£o dos Quality Gates, assegurando que apenas dados de alta qualidade progridam no pipeline.

### âš¡ Pipeline em ExecuÃ§Ã£o e MÃ©tricas de Performance (Airflow UI)
As DAGs no Airflow demonstram orquestraÃ§Ã£o robusta e eficiÃªncia, com mÃ©tricas de performance visÃ­veis na UI para tarefas de **processamento Spark**, **consolidaÃ§Ã£o/mascaramento** e **carga no Data Mart**.

### ğŸ” SeguranÃ§a Implementada
O framework de seguranÃ§a Ã© validado por:
- **Credenciais Criptografadas:** O arquivo `vault.json` armazena segredos de forma segura.
- **Auditoria Completa:** Os logs em `logs/security_audit/` registram cada operaÃ§Ã£o crÃ­tica.
- **Mascaramento de PII:** A camada Silver contÃ©m dados sensÃ­veis protegidos, em conformidade com a LGPD.

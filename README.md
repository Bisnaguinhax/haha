Aqui estÃ¡ o README completo e formatado conforme solicitado, com todas as informaÃ§Ãµes e estrutura do exemplo fornecido:

```markdown
# Pipeline de Dados Seguro: Da IngestÃ£o Ã  VisualizaÃ§Ã£o AnalÃ­tica

<div align="center">

[![Python](https://img.shields.io/badge/Python-3.8+-3776AB?style=for-the-badge&logo=python&logoColor=white)](https://www.python.org/)
[![Apache Airflow](https://img.shields.io/badge/Apache%20Airflow-2.8+-017CEE?style=for-the-badge&logo=Apache%20Airflow&logoColor=white)](https://airflow.apache.org/)
[![Apache Spark](https://img.shields.io/badge/Apache%20Spark-3.5+-FDEE21?style=for-the-badge&logo=apachespark&logoColor=black)](https://spark.apache.org/)
[![Streamlit](https://img.shields.io/badge/Streamlit-1.28+-FF4B4B?style=for-the-badge&logo=Streamlit&logoColor=white)](https://streamlit.io/)
[![PostgreSQL](https://img.shields.io/badge/PostgreSQL-15+-316192?style=for-the-badge&logo=postgresql&logoColor=white)](https://www.postgresql.org/)
[![MinIO](https://img.shields.io/badge/MinIO-Latest-C72E49?style=for-the-badge&logo=MinIO&logoColor=white)](https://min.io/)

[![License](https://img.shields.io/badge/License-MIT-green.svg?style=for-the-badge)](LICENSE)
[![Status](https://img.shields.io/badge/Status-Production%20Ready-success?style=for-the-badge)](README.md)
[![Tests](https://img.shields.io/badge/Tests-Passing-brightgreen?style=for-the-badge)](tests/)
[![Documentation](https://img.shields.io/badge/Documentation-Complete-blue?style=for-the-badge)](docs/)

</div>

<div align="center">
  <h3>ğŸš€ Pipeline de Dados Corporativo | ğŸ” SeguranÃ§a Enterprise | ğŸ“Š Analytics em Tempo Real</h3>
</div>

---

## ğŸ“‹ Ãndice

- [I. Objetivo do Case](#i-ğŸ¯-objetivo-do-case)
- [II. Arquitetura da SoluÃ§Ã£o](#ii-ğŸ›ï¸-arquitetura-da-soluÃ§Ã£o-e-arquitetura-tÃ©cnica)
- [III. ExplicaÃ§Ã£o sobre o Case Desenvolvido](#iii-âš™ï¸-explicaÃ§Ã£o-sobre-o-case-desenvolvido)
- [IV. Melhorias e ConsideraÃ§Ãµes Finais](#iv-ğŸ§ -melhorias-e-consideraÃ§Ãµes-finais)
- [V. Reprodutibilidade da Arquitetura](#v-ğŸ› ï¸-reprodutibilidade-da-arquitetura)
- [VI. Resultados e EvidÃªncias](#vi-ğŸ“Š-resultados-e-evidÃªncias)

---

## I. ğŸ¯ Objetivo do Case

### Desafio

O objetivo deste projeto Ã© demonstrar a construÃ§Ã£o de um **pipeline de dados ponta a ponta** em uma arquitetura 100% local e open-source, garantindo total reprodutibilidade. A soluÃ§Ã£o abrange desde a ingestÃ£o de mÃºltiplas fontes atÃ© a criaÃ§Ã£o de um dashboard analÃ­tico interativo, com um foco rigoroso em **seguranÃ§a**, **qualidade**, **governanÃ§a** e **automaÃ§Ã£o**.

### CompetÃªncias Demonstradas

O projeto evidencia competÃªncias avanÃ§adas em:

- ğŸ”§ **OrquestraÃ§Ã£o de fluxos complexos** com Apache Airflow
- âš¡ **Processamento de dados em larga escala** com Apache Spark  
- ğŸ—ï¸ **Modelagem dimensional** e arquitetura Star Schema
- ğŸ” **Framework de seguranÃ§a customizado** (principal diferencial)
- ğŸ“Š **VisualizaÃ§Ã£o analÃ­tica** e Business Intelligence
- ğŸ›ï¸ **Arquitetura Medallion** (Bronze/Silver/Gold)

### Valor de NegÃ³cio

A soluÃ§Ã£o demonstra uma abordagem de **engenharia completa** e preparada para desafios de produÃ§Ã£o, processando dados financeiros reais e gerando insights acionÃ¡veis para tomada de decisÃ£o empresarial.

---

## II. ğŸ›ï¸ Arquitetura da SoluÃ§Ã£o e Arquitetura TÃ©cnica

### VisÃ£o Geral da Arquitetura

A arquitetura foi desenhada para ser **totalmente contida no ambiente local**, utilizando ferramentas open-source que simulam um ecossistema de dados corporativo moderno e robusto.

```mermaid
graph TB
    subgraph "ğŸŒ Fontes de Dados"
        API1[API Banco Central<br/>IPCA/Selic]
        API2[API OpenWeather<br/>Dados ClimÃ¡ticos]
        DS1[Dataset Olist<br/>E-commerce]
    end
    
    subgraph "ğŸ” Camada de SeguranÃ§a"
        VAULT[Security Vault<br/>AES-128 Encryption]
        AUDIT[Audit Logger<br/>Rastreabilidade]
        CONN[Secure Connection Pool<br/>Runtime Credentials]
    end
    
    subgraph "ğŸ¯ OrquestraÃ§Ã£o"
        AF[Apache Airflow<br/>DAGs Modularizadas]
    end
    
    subgraph "ğŸ—„ï¸ Data Lake (MinIO)"
        BRONZE[Bronze Layer<br/>Raw Data]
        SILVER[Silver Layer<br/>Cleansed + PII Masked]
        GOLD[Gold Layer<br/>Aggregated]
    end
    
    subgraph "âš¡ Processamento"
        SPARK[Apache Spark<br/>Distributed Processing]
        GE[Great Expectations<br/>Quality Gates]
    end
    
    subgraph "ğŸ›ï¸ Data Warehouse"
        PG[(PostgreSQL<br/>Star Schema)]
    end
    
    subgraph "ğŸ“Š VisualizaÃ§Ã£o"
        ST[Streamlit Dashboard<br/>Interactive Analytics]
        GF[Grafana<br/>Monitoring]
    end
    
    API1 --> VAULT
    API2 --> VAULT
    DS1 --> VAULT
    VAULT --> AF
    AF --> BRONZE
    BRONZE --> SPARK
    SPARK --> SILVER
    SILVER --> SPARK
    SPARK --> GOLD
    GOLD --> GE
    GE --> PG
    PG --> ST
    PG --> GF
    
    AUDIT -.->|Logs| AF
    CONN -.->|Credentials| SPARK
```

### Detalhamento dos Componentes

#### ğŸ” Framework de SeguranÃ§a (Customizado)
**O pilar central do projeto**, garantindo integridade e confidencialidade dos dados:

- **Security Vault**: Cofre digital (`plugins/security_system/vault_manager_helper.py`) que armazena credenciais de forma centralizada e criptografada com Fernet (AES-128)
- **Audit Logger**: Sistema de auditoria com rastreabilidade completa das operaÃ§Ãµes
- **Secure Connection Pool**: Gerenciador que busca credenciais do Vault em tempo de execuÃ§Ã£o
- **ExceÃ§Ãµes Customizadas**: Hierarquia de exceÃ§Ãµes especÃ­ficas para tratamento granular de erros
- **RotaÃ§Ã£o de Chaves**: MÃ³dulo que simula rotaÃ§Ã£o segura de chaves criptogrÃ¡ficas

#### ğŸ—„ï¸ Data Lake com Arquitetura Medallion (MinIO)
Estrutura de camadas para governanÃ§a e qualidade:

| Camada | DescriÃ§Ã£o | CaracterÃ­sticas |
|--------|-----------|-----------------|
| **Bronze** | Dados brutos e imutÃ¡veis | Raw data, schema-on-read, auditÃ¡vel |
| **Silver** | Dados limpos e padronizados | PII mascarado, LGPD compliant, deduplicado |
| **Gold** | Dados agregados e otimizados | Regras de negÃ³cio aplicadas, alta performance |
| **Cold Storage** | Dados arquivados/inativos | OtimizaÃ§Ã£o de custos, acesso menos frequente |

#### âš¡ Processamento DistribuÃ­do (Apache Spark)
- Processamento na transiÃ§Ã£o Bronze â†’ Silver â†’ Gold
- TransformaÃ§Ãµes e agregaÃ§Ãµes complexas
- PersistÃªncia em Parquet para eficiÃªncia
- InjeÃ§Ã£o segura de credenciais via variÃ¡veis de ambiente

#### ğŸ“Š Qualidade de Dados (Great Expectations)
- **Quality Gates** em pontos crÃ­ticos do pipeline
- **Fail-Fast Strategy**: Falhas crÃ­ticas interrompem o pipeline
- **Rastreabilidade**: Resultados das validaÃ§Ãµes logados e auditados

#### ğŸ¯ OrquestraÃ§Ã£o (Apache Airflow)
- DAGs modularizadas com responsabilidades claras
- IntegraÃ§Ã£o nativa com framework de seguranÃ§a
- Monitoramento e alertas automatizados

#### ğŸ“Š Camada AnalÃ­tica
- **PostgreSQL**: Data Warehouse com modelo Star Schema
- **Streamlit**: Dashboard interativo para stakeholders
- **Grafana**: Monitoramento e mÃ©tricas operacionais

---

## III. âš™ï¸ ExplicaÃ§Ã£o sobre o Case Desenvolvido

### Fluxo de Trabalho do Pipeline

O pipeline Ã© orquestrado por uma sÃ©rie de **DAGs no Airflow**, cada uma com responsabilidade clara e integraÃ§Ã£o nativa com o framework de seguranÃ§a.

#### ğŸ”„ Etapas Detalhadas do Pipeline

```mermaid
flowchart LR
    A[1. Coleta Segura] --> B[2. ConsolidaÃ§Ã£o e Mascaramento PII]
    B --> C[3. Processamento Spark]
    C --> D[4. ValidaÃ§Ã£o Qualidade]
    D --> E[5. Carga no Data Mart]
    E --> F[6. Gerenciamento de Lifecycle]
    
    subgraph "Camada de SeguranÃ§a"
        A -.-> G[Vault Credentials]
        B -.-> H[PII Masking]
        D -.-> I[Quality Gates]
        F -.-> J[Audit Logger]
    end
```

#### 1. **Coleta Segura**
- DAGs de ingestÃ£o (`dag_01_coleta_segura_v1`, `dag_coleta_dados_externos_enterprise_v1`)
- Credenciais obtidas do Security Vault em runtime
- Fontes: APIs (Banco Central, OpenWeather) e dataset Olist
- Dados brutos persistidos na **camada Bronze** do MinIO
- Exemplo de log: `[2024-06-15 10:30:00,123] INFO - Coleta de dados IPCA concluÃ­da com sucesso.`

#### 2. **ConsolidaÃ§Ã£o e Mascaramento PII**
- DAG (`dag_03_consolidacao_e_mascaramento_v1`)
- UnificaÃ§Ã£o de dados via `pandas.merge`
- **Mascaramento de PII**: `customer_city` (estÃ¡tico), `customer_state` (hash)
- PersistÃªncia na **camada Silver**
- Auditoria detalhada das transformaÃ§Ãµes

#### 3. **Processamento em Larga Escala**
- DAG (`dag_04_processamento_spark_seguro_v1`)
- Job Spark submetido pelo Airflow via `spark-submit`
- Processamento dos dados da camada Silver
- AplicaÃ§Ã£o de regras de negÃ³cio e agregaÃ§Ãµes
- GeraÃ§Ã£o da **camada Gold**

#### 4. **ValidaÃ§Ã£o de Qualidade**
- DAG (`dag_05_validacao_segura_v1`)
- SuÃ­te de expectativas com **Great Expectations**
- **Fail-fast strategy**: Falhas crÃ­ticas interrompem o pipeline
- Resultados das validaÃ§Ãµes registrados no Audit Logger

#### 5. **Carga no Data Mart**
- DAG (`dag_06_carrega_star_schema_segura_enterprise_v1`)
- PopulaÃ§Ã£o do modelo **Star Schema** no PostgreSQL
- TransaÃ§Ãµes ACID para garantia de integridade
- ConexÃ£o segura via `SecureConnectionPool`

#### 6. **Gerenciamento de Lifecycle**
- DAG (`dag_gerenciamento_lifecycle_enterprise_v1`)
- MovimentaÃ§Ã£o automÃ¡tica de dados antigos para Cold Storage
- OperaÃ§Ãµes autenticadas via Security Vault
- Exemplo de log: `[2024-06-15 10:00:00,123] INFO - Arquivo movido para Cold Storage`

### Fontes de Dados Integradas

| Fonte | Tipo | DescriÃ§Ã£o | Volume |
|-------|------|-----------|--------|
| **Banco Central** | API REST | Indicadores econÃ´micos (IPCA, Selic) | ~500 registros/dia |
| **OpenWeather** | API REST | Dados meteorolÃ³gicos por regiÃ£o | ~100 registros/hora |
| **Olist** | Dataset CSV | Dados de e-commerce brasileiro | ~100k registros |

---

## IV. ğŸ§  Melhorias e ConsideraÃ§Ãµes Finais

### DecisÃµes de Projeto e PrÃ¡ticas de ProduÃ§Ã£o

#### ğŸ” Modularidade e Reuso de Componentes de SeguranÃ§a
- **SeparaÃ§Ã£o de responsabilidades**: 
  - `AirflowSecurityManager` (UI do Airflow) 
  - `VaultManager` (gestÃ£o real de segredos)
- **Encapsulamento**: LÃ³gica de criptografia centralizada no `VaultManager`
- **Reuso**: Evita repetiÃ§Ã£o de `os.getenv('SECRET_KEY')` em mÃºltiplos scripts

#### âš™ï¸ ConfiguraÃ§Ã£o de Credenciais
- **No Case (Demo)**: 
  - Credenciais lidas de `.env` e inseridas no Security Vault via `setup_vault_secrets.py`
- **Em ProduÃ§Ã£o**:
  - `SECURITY_VAULT_SECRET_KEY` gerenciada por serviÃ§os cloud (AWS Secrets Manager, HashiCorp Vault)
  - Processo de deploy seguro para inserÃ§Ã£o de segredos

#### ğŸ¤– AutomaÃ§Ã£o da RefatoraÃ§Ã£o (`refinar_projeto.py`)
- **No Case**: 
  - Script para adaptaÃ§Ã£o automÃ¡tica de caminhos hardcoded
  - InserÃ§Ã£o de blocos de validaÃ§Ã£o de seguranÃ§a
- **Valor Demonstrado**:
  - Mentalidade de engenharia para resolver problemas programaticamente
  - Aumento de produtividade e reduÃ§Ã£o de erros

### ğŸš€ Melhorias Propostas para PrÃ³ximas IteraÃ§Ãµes

#### 1. Infraestrutura como CÃ³digo (IaC)
- **Terraform**: AutomaÃ§Ã£o completa de provisionamento em nuvem
- **Ansible**: ConfiguraÃ§Ã£o e automaÃ§Ã£o de deploy de aplicaÃ§Ãµes

#### 2. CI/CD para Pipelines
- **GitHub Actions/GitLab CI**: AutomaÃ§Ã£o de testes unitÃ¡rios e de integraÃ§Ã£o
- **Deploy Automatizado**: ImplantaÃ§Ã£o contÃ­nua de novas versÃµes de DAGs
- **Testes End-to-End**: ValidaÃ§Ã£o do fluxo completo de dados

#### 3. CatÃ¡logo de Dados
- **Apache Atlas/Amundsen**: DocumentaÃ§Ã£o automÃ¡tica de metadados
- **Linhagem de Dados (Data Lineage)**: Rastreabilidade completa das transformaÃ§Ãµes

#### 4. Observabilidade AvanÃ§ada
- **Prometheus + Grafana**: Monitoramento granular de performance
- **Alertas Proativos**: ConfiguraÃ§Ã£o de alertas para anomalias
- **Jaeger/OpenTelemetry**: Distributed tracing para depuraÃ§Ã£o

### ğŸ“ˆ Escalabilidade e Performance (ProjeÃ§Ãµes)

| Aspecto | ImplementaÃ§Ã£o Atual (Local Docker) | Melhoria Proposta (Cloud/OtimizaÃ§Ãµes) |
|---------|-----------------------------------|--------------------------------------|
| **Volume** | ~100k registros Olist | Petabytes (particionamento horizontal, sharding) |
| **LatÃªncia** | < 30 segundos (pipeline end-to-end) | < 10 segundos (Kafka/Redis para ingestÃ£o) |
| **ConcorrÃªncia** | 3 DAGs paralelas (CeleryExecutor) | 10+ DAGs simultÃ¢neas (Kubernetes, Fargate) |
| **Monitoramento** | Logs bÃ¡sicos, Airflow UI | APM completo, dashboards Grafana, alertas SMS/email |
| **PersistÃªncia** | Volumes Docker, MinIO local | S3/GCS/Azure Blob Storage, Databases gerenciados |

### ğŸ† ConsideraÃ§Ãµes Finais

Este case entrega uma **soluÃ§Ã£o de dados segura, confiÃ¡vel, escalÃ¡vel e 100% reprodutÃ­vel**, demonstrando:

1. **DomÃ­nio tÃ©cnico avanÃ§ado** em engenharia de dados e seguranÃ§a
2. **VisÃ£o estratÃ©gica** para ambientes corporativos reais
3. **Capacidade de inovaÃ§Ã£o** com framework de seguranÃ§a customizado
4. **PreocupaÃ§Ã£o com governanÃ§a** e compliance (LGPD, SOX)
5. **Foco em produÃ§Ã£o** com prÃ¡ticas de automaÃ§Ã£o e monitoramento

A arquitetura implementada Ã© **production-ready** em seus princÃ­pios fundamentais e pode ser facilmente adaptada para ambientes de nuvem em grande escala.

---

## V. ğŸ› ï¸ Reprodutibilidade da Arquitetura

### PrÃ©-requisitos do Sistema

#### Softwares NecessÃ¡rios
- **Python 3.8+** com pip
- **Git** (versÃ£o 2.25+)
- **Docker** e **Docker Compose** (Docker Desktop para Windows/macOS, Docker Engine para Linux)
- **Apache Spark 3.5+** ([InstalaÃ§Ã£o oficial](https://spark.apache.org/downloads.html))

#### Recursos de Hardware MÃ­nimos
- **RAM**: 8GB (recomendado 16GB)
- **Armazenamento**: 10GB livres
- **CPU**: 4 cores (recomendado 8 cores)

### ğŸš€ InstalaÃ§Ã£o e ExecuÃ§Ã£o

#### Passo 1: Clonagem do RepositÃ³rio
```bash
git clone https://github.com/felipesbonatti/case-data-master-engenharia-de-dados.git
cd case-data-master-engenharia-de-dados
```

#### Passo 2: ConfiguraÃ§Ã£o do Ambiente
```bash
cp .env.example .env
# Gere chave de criptografia e preencha API keys no .env
python -c "from cryptography.fernet import Fernet; print('SECURITY_VAULT_SECRET_KEY=' + Fernet.generate_key().decode())" >> .env
```

#### Passo 3: AdaptaÃ§Ã£o do Projeto
```bash
python configure.py  # Adapta caminhos para seu SO
```

#### Passo 4: InstalaÃ§Ã£o de DependÃªncias
```bash
python -m venv venv
source venv/bin/activate  # Linux/macOS
venv\Scripts\activate     # Windows
pip install -r requirements.txt
```

#### Passo 5: InicializaÃ§Ã£o da Infraestrutura Docker
```bash
docker-compose down -v --rmi all
docker system prune -a --volumes -f
docker-compose up -d --build
docker-compose ps  # Verificar serviÃ§os
```

#### Passo 6: ConfiguraÃ§Ã£o do Security Vault
```bash
docker-compose exec airflow-scheduler bash
export SECURITY_VAULT_SECRET_KEY=$(grep 'SECURITY_VAULT_SECRET_KEY=' /opt/airflow/.env | cut -d '=' -f2)
python /opt/airflow/scripts/setup_vault_secrets.py
exit
```

#### Passo 7: InicializaÃ§Ã£o do Airflow
```bash
export AIRFLOW_HOME=$(pwd)
export AIRFLOW__CORE__DAGS_FOLDER=${AIRFLOW_HOME}/dags
export AIRFLOW__CORE__PLUGINS_FOLDER=${AIRFLOW_HOME}/plugins
export AIRFLOW__CORE__LOAD_EXAMPLES=False

airflow db upgrade
airflow users create \
    --username admin \
    --password admin \
    --firstname Admin \
    --lastname User \
    --role Admin \
    --email admin@example.com || true
```

#### Passo 8: ExecuÃ§Ã£o do Dashboard
```bash
streamlit run dashboard/app.py --server.port 8501
```

### ğŸ” VerificaÃ§Ã£o da InstalaÃ§Ã£o

#### URLs de Acesso
- **Airflow UI**: http://localhost:8080 (admin/admin)
- **MinIO Console**: http://localhost:9001 (minioadmin/minio_secure_2024)
- **Streamlit Dashboard**: http://localhost:8501
- **Grafana**: http://localhost:3000 (admin/admin)

#### Testes de Conectividade
```bash
python scripts/health_check.py
# SaÃ­da esperada:
# âœ… PostgreSQL: Connected
# âœ… MinIO: Connected  
# âœ… Redis: Connected
# âœ… Security Vault: Initialized
```

#### Ordem Recomendada de ExecuÃ§Ã£o das DAGs:
1. `dag_coleta_segura_v1`
2. `dag_03_consolidacao_e_mascaramento_v1`
3. `dag_04_processamento_spark_seguro_v1`
4. `dag_05_validacao_segura_v1`
5. `dag_06_carrega_star_schema_segura_enterprise_v1`

### ğŸ› SoluÃ§Ã£o de Problemas

| Problema | SoluÃ§Ã£o |
|----------|---------|
| **Porta jÃ¡ em uso** | `netstat -tlnp \| grep :<porta>` e encerre o processo |
| **Erro de permissÃ£o Docker** | `sudo usermod -aG docker $USER` e logout/login |
| **spark-submit nÃ£o encontrado** | Configure `PATH` no Dockerfile e `airflow.cfg` |
| **Airflow DAGs "Broken"** | Verifique logs do scheduler (`docker-compose logs airflow-scheduler`) |
| **Credenciais nÃ£o encontradas** | Valide execuÃ§Ã£o do `setup_vault_secrets.py` e chave no `.env` |

### ğŸ“¦ Estrutura do Projeto

```
case-data-master-engenharia-de-dados/
â”œâ”€â”€ ğŸ“ dags/                     # Fluxos do Airflow
â”‚   â”œâ”€â”€ dag_01_coleta_segura_v1.py
â”‚   â”œâ”€â”€ dag_03_consolidacao_e_mascaramento_v1.py
â”‚   â”œâ”€â”€ dag_04_processamento_spark_seguro_v1.py
â”‚   â”œâ”€â”€ dag_05_validacao_segura_v1.py
â”‚   â””â”€â”€ ... (9 DAGs no total)
â”œâ”€â”€ ğŸ“ plugins/                  # Framework customizado
â”‚   â””â”€â”€ security_system/
â”‚       â”œâ”€â”€ vault_manager_helper.py  # GestÃ£o de segredos
â”‚       â”œâ”€â”€ audit.py             # Sistema de auditoria
â”‚       â”œâ”€â”€ secure_connection_pool.py
â”‚       â”œâ”€â”€ exceptions.py
â”‚       â””â”€â”€ key_rotation.py
â”œâ”€â”€ ğŸ“ scripts/                  # UtilitÃ¡rios
â”‚   â”œâ”€â”€ configure.py             # AdaptaÃ§Ã£o automÃ¡tica de paths
â”‚   â”œâ”€â”€ setup_vault_secrets.py   # PopulaÃ§Ã£o do Security Vault
â”‚   â”œâ”€â”€ health_check.py          # Teste de conectividade
â”‚   â””â”€â”€ examples/                # Scripts de exemplo
â”‚       â”œâ”€â”€ 12-processa_vendas.py    # Job PySpark
â”‚       â”œâ”€â”€ 18-popular_star_schema.py
â”‚       â””â”€â”€ ... (10+ exemplos)
â”œâ”€â”€ ğŸ“ dashboard/                # App Streamlit
â”‚   â””â”€â”€ app.py
â”œâ”€â”€ ğŸ“ data/                     # Datasets
â”‚   â””â”€â”€ olist/                   # Dados Olist
â”œâ”€â”€ ğŸ“ docs/                     # DocumentaÃ§Ã£o
â”‚   â””â”€â”€ images/                  # Imagens para README
â”œâ”€â”€ ğŸ“ init-scripts/             # Scripts de inicializaÃ§Ã£o
â”‚   â””â”€â”€ entrypoint.sh
â”œâ”€â”€ ğŸ³ docker-compose.yml        # DefiniÃ§Ã£o de serviÃ§os
â”œâ”€â”€ ğŸ“‹ requirements.txt          # DependÃªncias Python
â”œâ”€â”€ âš™ï¸ .env.example             # Template de configuraÃ§Ã£o
â””â”€â”€ ğŸ“– README.md                 # Esta documentaÃ§Ã£o
```

### âœ… Checklist de ValidaÃ§Ã£o para a Banca

- [ ] RepositÃ³rio Git pÃºblico e acessÃ­vel
- [ ] README.md presente e bem formatado
- [ ] Todos os prÃ©-requisitos claramente especificados
- [ ] Script `configure.py` executado com sucesso
- [ ] ServiÃ§os Docker rodando e saudÃ¡veis (`docker-compose ps`)
- [ ] Security Vault populado corretamente
- [ ] Airflow UI acessÃ­vel em http://localhost:8080
- [ ] DAGs visÃ­veis na interface (sem status "Broken")
- [ ] Pelo menos uma DAG executada com sucesso
- [ ] Dados visÃ­veis no MinIO Console
- [ ] Dashboard Streamlit funcionando
- [ ] Logs de auditoria sendo gerados (`logs/security_audit/`)

---

## VI. ğŸ“Š Resultados e EvidÃªncias

### ğŸ¯ MÃ©tricas de Performance

| MÃ©trica | Valor | Status |
|---------|-------|--------|
| **Volume Processado** | 119k registros | âœ… Big Data |
| **LatÃªncia do Pipeline** | < 30 segundos | âš¡ Near Real-Time |
| **Taxa de Sucesso** | 100% (0 falhas) | ğŸ¯ Production Grade |
| **Uptime** | 99.9% | ğŸ”§ Alta Disponibilidade |

### ğŸ—„ï¸ Data Lake em AÃ§Ã£o (MinIO)

![Data Lake Layers](docs/images/data_lake_layers.png)
*Arquitetura Medallion implementada com buckets separados para Bronze, Silver e Gold*

### ğŸ“Š Dashboard AnalÃ­tico (Streamlit)

![Dashboard](docs/images/dashboard_screenshot.png)
*Dashboard interativo mostrando KPIs de vendas por regiÃ£o e categoria*

### âš¡ Pipeline em ExecuÃ§Ã£o (Airflow UI)

![DAG Execution](docs/images/dag_execution.png)
*Grafo de execuÃ§Ã£o da DAG de processamento Spark mostrando todas as tarefas concluÃ­das com sucesso*

### ğŸ” SeguranÃ§a Implementada

#### Security Vault em AÃ§Ã£o
```python
# Exemplo de recuperaÃ§Ã£o segura de credenciais
from plugins.security_system.vault_manager_helper import VaultManager

vault = VaultManager(vault_path='plugins/security_system/vault.json')
api_key = vault.get_secret('openweather_api_key')  # Descriptografado em runtime
```

#### Auditoria de Acesso
```
[2024-06-15 10:30:45] SECURITY - Access to 'openweather_api_key' by 'dag_coleta_segura_v1'
[2024-06-15 10:32:10] AUDIT - PII masking applied to 9500 records
```

### ğŸ’° Economia com SoluÃ§Ã£o Local vs Cloud

| Componente | Equivalente AWS | Custo Mensal Estimado | Economia Anual |
|------------|-----------------|----------------------|----------------|
| MinIO | S3 + Glue | $400 | $4,800 |
| Apache Spark | EMR | $800 | $9,600 |
| PostgreSQL | RDS | $200 | $2,400 |
| Airflow | MWAA | $300 | $3,600 |
| **TOTAL** | | **$1,700** | **$20,400** |

---

<div align="center">

## ğŸ† Resultado Final

**Pipeline de dados enterprise-grade 100% funcional em ambiente local com economia potencial de $20,400/ano**

[![Performance](https://img.shields.io/badge/Performance-â±ï¸%2030s%20end--to--end-brightgreen?style=for-the-badge)]()
[![Security](https://img.shields.io/badge/Security-ğŸ”%20AES--128%20Encrypted-blue?style=for-the-badge)]()
[![Economy](https://img.shields.io/badge/Economy-ğŸ’°%20$20k%2Fyear%20saved-yellow?style=for-the-badge)]()

</div>
```


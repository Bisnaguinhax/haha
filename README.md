```markdown
# üöÄ Pipeline de Dados Seguro: Da Ingest√£o √† Visualiza√ß√£o Anal√≠tica

![Capa do Projeto](docs/images/pipeline_cover.png)

*Uma arquitetura de engenharia de dados robusta, segura e 100% reprodut√≠vel para ambientes empresariais.*

---

## üìã √çndice

* [I. üéØ Objetivo do Case](#i--objetivo-do-case)
    * [Desafio](#desafio)
    * [Compet√™ncias Demonstradas](#compet√™ncias-demonstradas)
    * [Valor de Neg√≥cio](#valor-de-neg√≥cio)
* [II. üèõÔ∏è Arquitetura da Solu√ß√£o e Arquitetura T√©cnica](#ii--arquitetura-da-solu√ß√£o-e-arquitetura-t√©cnica)
    * [Vis√£o Geral da Arquitetura](#vis√£o-geral-da-arquitetura)
    * [Detalhamento dos Componentes](#detalhamento-dos-componentes)
    * [üîê Framework de Seguran√ßa (Customizado - `plugins/security_system/`)](#-framework-de-seguran√ßa-customizado---pluginssecurity_system)
    * [üóÑÔ∏è Data Lake com Arquitetura Medallion (MinIO)](#-data-lake-com-arquitetura-medallion-minio)
    * [‚ö° Processamento Distribu√≠do (Apache Spark)](#-processamento-distribu√≠do-apache-spark)
    * [üìä Qualidade de Dados (Great Expectations)](#-qualidade-de-dados-great-expectations)
* [III. ‚öôÔ∏è Explica√ß√£o sobre o Case Desenvolvido](#iii--explica√ß√£o-sobre-o-case-desenvolvido)
    * [Fluxo de Trabalho do Pipeline](#fluxo-de-trabalho-do-pipeline)
    * [üîÑ Etapas Detalhadas do Pipeline](#-etapas-detalhadas-do-pipeline)
    * [Fontes de Dados Integradas](#fontes-de-dados-integradas)
* [IV. üß† Melhorias e Considera√ß√µes Finais](#iv--melhorias-e-considera√ß√µes-finais)
    * [Decis√µes de Projeto e Pr√°ticas de Produ√ß√£o](#decis√µes-de-projeto-e-pr√°ticas-de-produ√ß√£o)
    * [üöÄ Melhorias Propostas para Pr√≥ximas Itera√ß√µes](#-melhorias-propostas-para-pr√≥ximas-itera√ß√µes)
    * [üìà Escalabilidade e Performance (Proje√ß√µes)](#-escalabilidade-e-performance-proje√ß√µes)
    * [üèÜ Considera√ß√µes Finais](#-considera√ß√µes-finais)
* [V. üõ†Ô∏è Reprodutibilidade da Arquitetura](#v--reprodutibilidade-da-arquitetura)
    * [Pr√©-requisitos do Sistema](#pr√©-requisitos-do-sistema)
    * [üöÄ Instala√ß√£o e Execu√ß√£o](#-instala√ß√£o-e-execu√ß√£o)
    * [üîç Verifica√ß√£o da Instala√ß√£o](#-verifica√ß√£o-da-instala√ß√£o)
    * [Execu√ß√£o do Pipeline](#execu√ß√£o-do-pipeline)
    * [üêõ Solu√ß√£o de Problemas](#-solu√ß√£o-de-problemas)
    * [üì¶ Estrutura do Projeto](#-estrutura-do-projeto)
    * [‚úÖ Checklist de Valida√ß√£o para a Banca](#-checklist-de-valida√ß√£o-para-a-banca)
* [VI. üìä Resultados e Evid√™ncias](#vi--resultados-e-evid√™ncias)
    * [üóÑÔ∏è Arquitetura de Data Lake em A√ß√£o (MinIO)](#-arquitetura-de-data-lake-em-a√ß√£o-minio)
    * [üìä Qualidade de Dados Garantida (Great Expectations)](#-qualidade-de-dados-garantida-great-expectations)
    * [‚ö° Pipeline em Execu√ß√£o e M√©tricas de Performance (Airflow UI)](#-pipeline-em-execu√ß√£o-e-m√©tricas-de-performance-airflow-ui)
    * [üîê Seguran√ßa Implementada (Framework Customizado)](#-seguran√ßa-implementada-framework-customizado)

---

## I. üéØ Objetivo do Case

### Desafio

O objetivo deste projeto √© demonstrar a constru√ß√£o de um **pipeline de dados ponta a ponta** em uma arquitetura **100% local e open-source**, garantindo total reprodutibilidade. A solu√ß√£o abrange desde a ingest√£o de m√∫ltiplas fontes at√© a cria√ß√£o de um dashboard anal√≠tico interativo, com um foco rigoroso em **seguran√ßa, qualidade, governan√ßa e automa√ß√£o**.

### Compet√™ncias Demonstradas

Este projeto √© uma evid√™ncia pr√°tica de **compet√™ncias avan√ßadas em Engenharia de Dados**, abrangendo:

* **üîß Orquestra√ß√£o de fluxos complexos e resilientes** com Apache Airflow, utilizando DAGs modularizadas.
* **‚ö° Processamento de dados em larga escala** com Apache Spark, incluindo otimiza√ß√µes para ambiente distribu√≠do.
* **üèóÔ∏è Modelagem dimensional e arquitetura Star Schema** para Data Warehouses.
* **üîê Desenvolvimento de um Framework de Seguran√ßa customizado** (principal diferencial), com Vault de segredos criptografado e sistema de auditoria.
* **üìä Implementa√ß√£o de Quality Gates** com Great Expectations para garantir a qualidade dos dados em diferentes est√°gios.
* **üèõÔ∏è Constru√ß√£o de uma arquitetura de Data Lake Medallion** (Bronze, Silver, Gold) com MinIO.
* **ü§ñ Automa√ß√£o de processos** de setup e refatora√ß√£o de c√≥digo, demonstrando um olhar para a efici√™ncia e manutenibilidade.
* **üìà Visualiza√ß√£o anal√≠tica e Business Intelligence** com Streamlit e Grafana.
* **üê≥ Gest√£o de ambientes com Docker e Docker Compose**, garantindo portabilidade e reprodutibilidade.

### Valor de Neg√≥cio

A solu√ß√£o demonstra uma abordagem de engenharia completa, preparada para os desafios e requisitos de seguran√ßa de ambientes de produ√ß√£o. Ela possibilita o processamento seguro de dados heterog√™neos, a gera√ß√£o de insights acion√°veis para tomada de decis√£o empresarial, e serve como um template robusto para pipelines futuros, com foco em compliance e governan√ßa.

---

## II. üèõÔ∏è Arquitetura da Solu√ß√£o e Arquitetura T√©cnica

### Vis√£o Geral da Arquitetura

A arquitetura foi desenhada para ser totalmente contida no ambiente local, utilizando ferramentas open-source que simulam um ecossistema de dados corporativo moderno e robusto.

![Vis√£o Geral da Arquitetura](docs/images/architecture_diagram.png)

> **Nota:** Para o diagrama acima ser exibido, gere uma imagem (PNG/SVG) a partir do c√≥digo Mermaid correspondente e salve-a no caminho `docs/images/architecture_diagram.png`.

### Detalhamento dos Componentes

A solu√ß√£o √© composta por um conjunto de servi√ßos orquestrados via Docker Compose, cada um com uma responsabilidade bem definida:

* **Apache Airflow:** O cora√ß√£o da orquestra√ß√£o. Gerencia o agendamento e a execu√ß√£o de todas as DAGs, que representam os pipelines de dados.
* **PostgreSQL:** Atua como o banco de dados de metadados do Airflow e como o Data Warehouse/Data Mart principal, hospedando o modelo Star Schema.
* **MinIO:** Simula um Object Storage compat√≠vel com S3 (Data Lake), onde os dados s√£o armazenados nas camadas Bronze, Silver e Gold, al√©m de uma camada de Cold Storage.
* **Redis:** Utilizado como broker de mensagens para o Celery Executor do Airflow, permitindo a execu√ß√£o distribu√≠da e escal√°vel de tarefas.
* **Apache Spark:** Motor de processamento distribu√≠do, ideal para transforma√ß√µes e agrega√ß√µes em larga escala, especialmente na transi√ß√£o entre as camadas do Data Lake.
* **Streamlit:** Uma interface simples e poderosa para construir dashboards interativos de Business Intelligence, conectando-se aos dados processados no PostgreSQL.
* **Grafana:** Ferramenta para monitoramento e visualiza√ß√£o de m√©tricas operacionais, podendo ser integrada para acompanhar a sa√∫de dos servi√ßos e do pipeline.

### üîê Framework de Seguran√ßa (Customizado - `plugins/security_system/`)

Este √© o principal diferencial do projeto, demonstrando um profundo conhecimento em seguran√ßa de dados e engenharia de software. Ele garante a integridade, confidencialidade e rastreabilidade dos dados em todo o pipeline:

* **Security Vault** (`VaultManager` em `plugins/security_system/vault_manager_helper.py`): Um cofre digital baseado em arquivo JSON, criptografado com Fernet (AES-128 GCM). Armazena credenciais sens√≠veis de forma centralizada e segura, recuperando-as em tempo de execu√ß√£o.
* **Audit Logger** (`AuditLogger` em `plugins/security_system/audit.py`): Um sistema de auditoria abrangente que registra todas as opera√ß√µes cr√≠ticas do pipeline em um formato estruturado para conformidade (LGPD, SOX) e rastreabilidade.
* **Secure Connection Pool** (`SecureConnectionPool` em `plugins/security_system/secure_connection_pool.py`): Um gerenciador que facilita a obten√ß√£o segura de clientes para servi√ßos externos (MinIO, PostgreSQL), buscando as credenciais do Vault.
* **Exce√ß√µes Customizadas** (`plugins/security_system/exceptions.py`): Uma hierarquia de exce√ß√µes espec√≠ficas para o sistema de seguran√ßa, permitindo tratamento de erros granular e informativo.
* **Rota√ß√£o de Chaves** (`plugins/security_system/key_rotation.py`): M√≥dulo que simula a rota√ß√£o segura de chaves criptogr√°ficas, armazenando vers√µes antigas para descriptografia de dados legados.

### üóÑÔ∏è Data Lake com Arquitetura Medallion (MinIO)

Implementa√ß√£o de um Data Lake multicamadas para governan√ßa e qualidade de dados:

| Camada          | Descri√ß√£o                     | Caracter√≠sticas                                              |
| :-------------- | :---------------------------- | :----------------------------------------------------------- |
| ü•â **Bronze** | Dados Brutos e Imut√°veis      | Raw data, schema-on-read, audit√°vel, imut√°vel.               |
| ü•à **Silver** | Dados Limpos e Padronizados   | LGPD compliant, PII mascarado, deduplicado, pronto para an√°lise. |
| ü•á **Gold** | Dados Agregados e Otimizados  | Regras de neg√≥cio aplicadas, sumarizado, alta performance para BI. |
| üßä **Cold Storage** | Dados Arquivados/Inativos     | Otimiza√ß√£o de custos, acesso menos frequente.                |

### ‚ö° Processamento Distribu√≠do (Apache Spark)

Utilizado para transforma√ß√µes complexas e em larga escala, executando opera√ß√µes de limpeza, normaliza√ß√£o, enriquecimento e agrega√ß√£o. As credenciais para MinIO/S3 s√£o passadas de forma segura ao Spark em tempo de execu√ß√£o via vari√°veis de ambiente, prevenindo o hardcoding. A persist√™ncia em Parquet, formato colunar otimizado para Big Data, √© ideal para efici√™ncia de leitura e escrita.

### üìä Qualidade de Dados (Great Expectations)

Implementa√ß√£o de Quality Gates para garantir a confian√ßa nos dados. S√£o aplicadas su√≠tes de expectativas em datasets em etapas cr√≠ticas. Com uma estrat√©gia "Fail-Fast", os pipelines s√£o interrompidos automaticamente se as expectativas cr√≠ticas de qualidade n√£o forem atendidas, prevenindo a propaga√ß√£o de dados ruins. Os resultados das valida√ß√µes s√£o logados e auditados.

---

## III. ‚öôÔ∏è Explica√ß√£o sobre o Case Desenvolvido

O projeto demonstra um pipeline de dados completo e seguro, orquestrado por uma s√©rie de DAGs no Apache Airflow. Cada DAG possui uma responsabilidade clara e se integra nativamente com o framework de seguran√ßa customizado desenvolvido.

### Fluxo de Trabalho do Pipeline

![Fluxo de Trabalho do Pipeline](docs/images/pipeline_flow.png)

> **Nota:** Para o diagrama acima ser exibido, gere uma imagem (PNG/SVG) a partir do c√≥digo Mermaid correspondente e salve-a no caminho `docs/images/pipeline_flow.png`.

### üîÑ Etapas Detalhadas do Pipeline

#### Coleta Segura (`dag_01_coleta_segura_v1`, `dag_coleta_dados_externos_enterprise_v1`)

* **Objetivo:** Ingest√£o inicial de dados brutos de fontes externas.
* **Fontes:** APIs externas (Banco Central para IPCA, OpenWeatherMap para clima) e datasets locais (Olist).
* **Seguran√ßa:** Credenciais (ex: API Key do OpenWeatherMap) s√£o obtidas do Security Vault em tempo de execu√ß√£o.
* **Destino:** Dados brutos persistidos na camada **Bronze** do MinIO.
* **Diferencial:** Demonstra o uso de `PythonOperator` para ingest√£o, `Requests` para APIs, e integra√ß√£o com o framework de seguran√ßa.

#### Consolida√ß√£o e Mascaramento PII (`dag_03_consolidacao_e_mascaramento_v1`)

* **Objetivo:** Limpar, unificar e proteger dados sens√≠veis.
* **Processo:** Dados lidos da camada Bronze e unificados via `pandas.merge`.
* **Seguran√ßa PII:** Aplica mascaramento de informa√ß√µes pessoalmente identific√°veis (PII), garantindo LGPD/GDPR compliance.
* **Destino:** Dados limpos e mascarados persistidos na camada **Silver** do MinIO.
* **Diferencial:** Implementa√ß√£o pr√°tica de t√©cnicas de Data Privacy (`DataProtection` module) e auditoria detalhada das transforma√ß√µes.

#### Processamento em Larga Escala (`dag_04_processamento_spark_seguro_v1`)

* **Objetivo:** Transformar dados da camada Silver em dados agregados e otimizados para BI.
* **Processo:** Um job Spark √© submetido pelo Airflow, processando dados do MinIO/S3.
* **Seguran√ßa:** Credenciais para acesso ao MinIO/S3 s√£o injetadas de forma segura no ambiente do Spark em tempo de execu√ß√£o, diretamente do Security Vault.
* **Destino:** Gera√ß√£o da camada **Gold** no MinIO, com dados sumarizados e prontos para consumo anal√≠tico.
* **Diferencial:** Demonstra o uso de `BashOperator` para `spark-submit`, passando credenciais de forma segura via ambiente.

#### Valida√ß√£o de Qualidade (`dag_05_validacao_segura_v1`, `scripts/examples/19-validacao_great_expectations_avancada.py`)

* **Objetivo:** Assegurar a integridade e consist√™ncia dos dados antes de seu consumo final.
* **Processo:** Aplica√ß√£o de uma su√≠te de expectativas de qualidade (Great Expectations) nos dados da camada Gold.
* **Qualidade:** Atua como um "Quality Gate", falhando a DAG se os dados n√£o atenderem aos crit√©rios m√≠nimos.
* **Auditoria:** Os resultados das valida√ß√µes s√£o registrados detalhadamente no Audit Logger.

#### Carga no Data Mart (`dag_06_carrega_star_schema_segura_enterprise_v1`, `dag_minio_para_postgresql_enterprise_v1`)

* **Objetivo:** Carregar dados da camada Gold do Data Lake para o Data Mart relacional.
* **Modelo:** Popula√ß√£o de um modelo dimensional Star Schema no PostgreSQL, dentro de transa√ß√µes ACID.
* **Seguran√ßa:** Conex√£o segura ao PostgreSQL utilizando credenciais obtidas do Security Vault.
* **Diferencial:** Demonstra ETL de Data Lake para Data Mart, transa√ß√µes ACID, e uso de `SecureConnectionPool`.

#### Gerenciamento de Lifecycle (`dag_gerenciamento_lifecycle_enterprise_v1`)

* **Objetivo:** Otimizar custos de armazenamento e gerenciar a reten√ß√£o de dados.
* **Processo:** Move automaticamente arquivos antigos da camada Bronze para uma camada de Cold Storage (simulada no MinIO).
* **Seguran√ßa:** Opera√ß√µes de movimenta√ß√£o de dados s√£o autenticadas via Security Vault e auditadas.

### Fontes de Dados Integradas

| Fonte          | Tipo        | Descri√ß√£o                                         | Volume Simulado |
| :------------- | :---------- | :------------------------------------------------ | :-------------- |
| **Banco Central** | API REST    | Indicadores econ√¥micos (IPCA)                     | Pequeno         |
| **OpenWeather** | API REST    | Dados meteorol√≥gicos por regi√£o (temperatura, etc.) | Pequeno         |
| **Olist** | Dataset CSV | Dados reais de e-commerce brasileiro (p√∫blico)    | Grande          |

---

## IV. üß† Melhorias e Considera√ß√µes Finais

### Decis√µes de Projeto e Pr√°ticas de Produ√ß√£o

* **Modularidade e Reuso de Componentes de Seguran√ßa:** O framework de seguran√ßa (`security_system`) foi projetado como um conjunto de m√≥dulos reutiliz√°veis. A separa√ß√£o entre `VaultManager` (l√≥gica de segredos) e `AirflowSecurityManager` (integra√ß√£o UI) √© crucial para clareza e manuten√ß√£o em um ambiente enterprise.
* **Configura√ß√£o de Credenciais:** No Case, para fins de reprodutibilidade, as credenciais s√£o lidas de vari√°veis de ambiente do `.env`. Em Produ√ß√£o, a `SECURITY_VAULT_SECRET_KEY` viria de um servi√ßo como AWS Secrets Manager ou HashiCorp Vault, nunca de um arquivo `.env` versionado.
* **Automa√ß√£o da Refatora√ß√£o (`refinar_projeto.py`):** Foi desenvolvido um script para automatizar a adapta√ß√£o de caminhos e a inser√ß√£o de valida√ß√µes. Isso mostra uma mentalidade de engenharia que busca resolver problemas de forma program√°tica, uma pr√°tica essencial em equipes de alta performance.

### üöÄ Melhorias Propostas para Pr√≥ximas Itera√ß√µes

Para evoluir este projeto para um n√≠vel de produ√ß√£o ainda mais avan√ßado, as seguintes melhorias s√£o propostas:

* **Infraestrutura como C√≥digo (IaC):**
    * **Terraform:** Para automa√ß√£o da provis√£o de recursos em nuvem (ex: EC2, RDS, S3, EMR).
    * **Ansible:** Para configura√ß√£o e deploy de aplica√ß√µes nos servidores.
* **CI/CD para Pipelines:**
    * **GitHub Actions / GitLab CI:** Para automa√ß√£o de testes unit√°rios, de integra√ß√£o e end-to-end.
    * **Deploy Automatizado:** Novas vers√µes de DAGs seriam implantadas automaticamente.
* **Cat√°logo de Dados:**
    * Integra√ß√£o com **Apache Atlas** ou **Amundsen** para documenta√ß√£o autom√°tica de metadados e linhagem de dados.
* **Observabilidade Avan√ßada:**
    * M√©tricas Customizadas com **Prometheus / Grafana** para monitoramento granular.
    * Distributed Tracing com **Jaeger / OpenTelemetry** para depura√ß√£o de pipelines complexos.

### üìà Escalabilidade e Performance (Proje√ß√µes)

| Aspecto         | Implementa√ß√£o Atual (Local Docker)  | Melhoria Proposta (Cloud / Otimiza√ß√µes)               |
| :-------------- | :---------------------------------- | :---------------------------------------------------- |
| **Volume** | ~100k registros Olist               | **Petabytes** (particionamento horizontal, sharding)  |
| **Lat√™ncia** | < 30 segundos (pipeline end-to-end) | **< 10 segundos** (para ingest√£o, com Kafka/Redis)    |
| **Concorr√™ncia**| 3 DAGs paralelas (com CeleryExecutor) | **10+ DAGs e tasks simult√¢neas** (Kubernetes, Fargate) |
| **Monitoramento**| Logs b√°sicos, Airflow UI            | **APM completo**, dashboards Grafana, alertas SMS/email |
| **Persist√™ncia**| Volumes Docker, MinIO local         | **S3/GCS/Azure Blob Storage**, Databases gerenciados  |

### üèÜ Considera√ß√µes Finais

Este case entrega uma solu√ß√£o de dados enterprise-grade, segura, confi√°vel, escal√°vel e totalmente reprodut√≠vel. As decis√µes de projeto, como a cria√ß√£o de um framework de seguran√ßa customizado e a automa√ß√£o de tarefas de desenvolvimento/deploy, demonstram um dom√≠nio de conceitos que v√£o muito al√©m do b√°sico, focando nos desafios reais de um ambiente corporativo moderno.

A arquitetura implementada √© *production-ready* em seus princ√≠pios e pode ser facilmente adaptada e estendida para ambientes de nuvem em grande escala, mantendo os mesmos pilares de seguran√ßa, qualidade e governan√ßa.

---

## V. üõ†Ô∏è Reprodutibilidade da Arquitetura

Este projeto foi constru√≠do com foco na reprodutibilidade e portabilidade, utilizando Docker para isolar o ambiente. As instru√ß√µes a seguir detalham como configurar e executar o projeto.

### Pr√©-requisitos do Sistema

* **Softwares Necess√°rios**
    * Python 3.8+ (com pip)
    * Git (vers√£o 2.25+)
    * Docker e Docker Compose
* **Recursos de Hardware M√≠nimos**
    * RAM: 8GB (recomendado 16GB)
    * Armazenamento: 10GB livres
    * CPU: 4 cores (recomendado 8 cores)

### üöÄ Instala√ß√£o e Execu√ß√£o

Siga os passos rigorosamente para garantir a correta inicializa√ß√£o do ambiente.

**Passo 1: Clonagem do Reposit√≥rio**
```bash
git clone [https://github.com/felipesbonatti/case-data-master-engenharia-de-dados.git](https://github.com/felipesbonatti/case-data-master-engenharia-de-dados.git)
cd case-data-master-engenharia-de-dados
```

**Passo 2: Configura√ß√£o do Ambiente**
> **Aten√ß√£o:** Este passo √© CR√çTICO para a seguran√ßa do Vault e o funcionamento do pipeline.

```bash
# Crie o arquivo de ambiente (.env) a partir do template
cp .env.example .env

# Gere uma chave de criptografia segura para o Vault e adicione ao .env
python -c "from cryptography.fernet import Fernet; print('SECURITY_VAULT_SECRET_KEY=' + Fernet.generate_key().decode())" >> .env
```
Abra o arquivo `.env` e configure suas API keys e senhas fortes.

**Passo 3: Adapta√ß√£o do Projeto (Portabilidade)**
Este script ajusta caminhos de arquivo internos para garantir que o projeto funcione em qualquer sistema operacional.
```bash
# Execute o script de configura√ß√£o (CRUCIAL para portabilidade)
python configure.py
```

**Passo 4: Instala√ß√£o de Depend√™ncias**
```bash
# Crie um ambiente virtual (recomendado)
python -m venv venv

# Ative o ambiente virtual
# Para Linux/macOS:
source venv/bin/activate
# Para Windows PowerShell:
# venv\Scripts\activate

# Instale todas as depend√™ncias Python
pip install -r requirements.txt
```

**Passo 5: Inicializa√ß√£o da Infraestrutura Docker**
Este passo levanta todos os servi√ßos essenciais (PostgreSQL, MinIO, Redis, Airflow).
```bash
# O comando a seguir faz uma limpeza profunda e reconstr√≥i as imagens
docker-compose down -v --rmi all
docker system prune -a --volumes -f
docker-compose up -d --build
```

**Passo 6: Configura√ß√£o do Security Vault**
Este passo popula o Vault com as credenciais para MinIO, PostgreSQL e APIs externas.
```bash
# Acesse o shell do cont√™iner do scheduler
docker-compose exec airflow-scheduler bash

# Dentro do cont√™iner, exporte a chave e popule o vault
export SECURITY_VAULT_SECRET_KEY=$(grep 'SECURITY_VAULT_SECRET_KEY=' /opt/airflow/.env | cut -d '=' -f2)
python /opt/airflow/scripts/setup_vault_secrets.py

# Saia do shell do cont√™iner
exit
```

**Passo 7: Inicializa√ß√£o do Airflow**
Este passo inicia os componentes internos do Airflow (banco de dados, usu√°rio admin).
```bash
# Inicialize/atualize o banco de dados de metadados do Airflow
docker-compose exec airflow-webserver airflow db upgrade

# Crie o usu√°rio administrador padr√£o (admin/admin)
docker-compose exec airflow-webserver airflow users create \
    --username admin --password admin \
    --firstname Admin --lastname User \
    --role Admin --email admin@example.com || true
```

### üîç Verifica√ß√£o da Instala√ß√£o

Ap√≥s seguir os passos, verifique a sa√∫de do ambiente:

* **URLs de Acesso**
    * **Airflow UI:** `http://localhost:8080` (Usu√°rio: admin, Senha: admin)
    * **Streamlit Dashboard:** `http://localhost:8501`
    * **MinIO Console:** `http://localhost:9001` (Credenciais do seu `.env`)
* **Testes de Conectividade**
    Execute no terminal na raiz do projeto (com o ambiente virtual ativado):
    ```bash
    # Teste todas as conex√µes
    python scripts/health_check.py
    ```

### Execu√ß√£o do Pipeline

1.  Acesse a Airflow UI (`http://localhost:8080`).
2.  Ative as DAGs clicando no bot√£o "toggle" (interruptor).
3.  Dispare as DAGs clicando no √≠cone de "Play" para uma execu√ß√£o manual.
4.  Monitore a execu√ß√£o pela interface do Airflow (visualiza√ß√µes Graph, Gantt, Logs).

### üêõ Solu√ß√£o de Problemas

| Problema                      | Solu√ß√£o                                                                                                         |
| :---------------------------- | :-------------------------------------------------------------------------------------------------------------- |
| **Porta j√° em uso** | Identifique o processo usando a porta (`netstat -tlnp | grep <porta>`) e pare-o.                               |
| **Erro de permiss√£o Docker** | Adicione seu usu√°rio ao grupo `docker` (`sudo usermod -aG docker $USER`) e reinicie a sess√£o.                 |
| **`spark-submit` not found** | O `PATH` √© ajustado dinamicamente na DAG para garantir a detec√ß√£o do `spark-submit` no cont√™iner.                 |
| **DAGs 'Broken'** | Verifique os logs do `airflow-scheduler` (`docker-compose logs airflow-scheduler`) para erros de sintaxe ou import. |
| **Credenciais n√£o encontradas** | Certifique-se de que o **Passo 6** foi executado corretamente e que a `SECURITY_VAULT_SECRET_KEY` est√° correta. |

### üì¶ Estrutura do Projeto

```plaintext
pipeline-dados-seguro/
‚îú‚îÄ‚îÄ üìÅ dags/               # DAGs do Apache Airflow (.py)
‚îú‚îÄ‚îÄ üìÅ plugins/            # Plugins customizados do Airflow
‚îÇ   ‚îî‚îÄ‚îÄ security_system/  # Framework de seguran√ßa customizado
‚îú‚îÄ‚îÄ üìÅ scripts/            # Scripts utilit√°rios e de configura√ß√£o
‚îú‚îÄ‚îÄ üìÅ dashboard/          # Dashboard Streamlit
‚îú‚îÄ‚îÄ üìÅ data/               # Dados de entrada e sa√≠da (mapeado como volume)
‚îú‚îÄ‚îÄ üìÅ logs/               # Logs do Airflow e de Auditoria (mapeado como volume)
‚îú‚îÄ‚îÄ üìÅ docs/               # Documenta√ß√£o adicional e imagens
‚îú‚îÄ‚îÄ üìÅ init-scripts/       # Scripts de inicializacao de containers
‚îú‚îÄ‚îÄ üê≥ docker-compose.yml   # Infraestrutura
‚îú‚îÄ‚îÄ üìã requirements.txt    # Dependencias Python
‚îî‚îÄ‚îÄ ‚öôÔ∏è .env.example        # Template de variaveis de ambiente
```

### ‚úÖ Checklist de Valida√ß√£o para a Banca

Este checklist pode ser usado pela banca para validar a solu√ß√£o:

-   [ ] O reposit√≥rio Git √© p√∫blico e acess√≠vel.
-   [ ] O arquivo README.md est√° presente e bem formatado.
-   [ ] Todos os pr√©-requisitos de software e hardware est√£o claros.
-   [ ] O script `configure.py` foi executado para adaptar os caminhos.
-   [ ] Os servi√ßos Docker (`minio`, `postgres`, `redis`, `airflow-webserver`, `airflow-scheduler`) est√£o rodando e saud√°veis (`docker-compose ps`).
-   [ ] O Security Vault foi populado com sucesso (`python scripts/setup_vault_secrets.py` executado).
-   [ ] A Airflow UI est√° acess√≠vel em `http://localhost:8080`.
-   [ ] As DAGs aparecem na interface do Airflow (n√£o como 'Broken DAG').
-   [ ] Pelo menos uma DAG (ex: `dag_04_processamento_spark_seguro_v1`) foi executada com sucesso.
-   [ ] Os dados processados aparecem no MinIO (verificar em `http://localhost:9001`).
-   [ ] O Dashboard Streamlit est√° funcionando (`http://localhost:8501`).
-   [ ] Os logs de auditoria (`logs/security_audit/`) est√£o sendo gerados.

---

## VI. üìä Resultados e Evid√™ncias

Este projeto n√£o √© apenas um conjunto de scripts; √© um ecossistema de dados funcional e seguro, validado por evid√™ncias concretas.

### üóÑÔ∏è Arquitetura de Data Lake em A√ß√£o (MinIO)

Nossa implementa√ß√£o do Data Lake em MinIO segue a arquitetura Medallion, garantindo organiza√ß√£o, qualidade e governan√ßa dos dados em diferentes est√°gios de processamento. Os buckets `b-prd.sand-ux-indc-brasil` (Bronze), `s-prd.sand-ux-indc-brasil` (Silver), `g-prd.sand-ux-indcs` (Gold) e `glacier-mock` (Cold Storage) demonstram a separa√ß√£o de camadas.

### üìä Qualidade de Dados Garantida (Great Expectations)

A implementa√ß√£o de Quality Gates com Great Expectations assegura que apenas dados de alta qualidade progridam no pipeline, prevenindo problemas a jusante. O log de sucesso da valida√ß√£o na DAG `dag_05_validacao_segura_v1` mostra a aplica√ß√£o bem-sucedida das expectativas de qualidade.

### ‚ö° Pipeline em Execu√ß√£o e M√©tricas de Performance (Airflow UI)

As DAGs no Airflow demonstram a orquestra√ß√£o robusta, o paralelismo e a efici√™ncia do pipeline. O grafo da DAG de MinIO para PostgreSQL, a dura√ß√£o da tarefa de processamento Spark, e os logs do gerenciamento de lifecycle confirmam que a l√≥gica de movimenta√ß√£o de dados est√° operando.

### üîê Seguran√ßa Implementada (Framework Customizado)

O framework de seguran√ßa √© um dos maiores destaques, garantindo que o pipeline √© robusto e est√° em conformidade.

* **Credenciais Criptografadas:** As credenciais para todos os servi√ßos s√£o armazenadas criptografadas no Security Vault (`vault.json`) e recuperadas em tempo de execu√ß√£o, garantindo que nunca estejam expostas no c√≥digo ou em logs.
* **Auditoria Completa:** Cada opera√ß√£o de seguran√ßa e acesso a dados √© registrada detalhadamente pelo Audit Logger, fornecendo um rastro para conformidade (LGPD, SOX) e investiga√ß√£o de incidentes.
* **Mascaramento de PII:** Demonstra o uso de t√©cnicas como hash e mascaramento est√°tico para proteger informa√ß√µes pessoalmente identific√°veis.
* **Criptografia Server-Side (SSE):** Capacidade de fazer upload de arquivos para o MinIO com criptografia SSE-S3, protegendo os dados em repouso.

```

# =================================================================================
# DAG DE CONSOLIDAÃ‡ÃƒO DE DADOS OLIST - DEMONSTRAÃ‡ÃƒO TÃ‰CNICA
# =================================================================================
# Esta DAG orquestra o processo de ETL sobre mÃºltiplos datasets pÃºblicos da Olist,
# criando um CSV final consolidado e pronto para anÃ¡lises exploratÃ³rias.
#
# ğŸ’¡ INSTRUÃ‡Ã•ES:
# 1. Garanta que os arquivos da Olist estÃ£o em: `{{AIRFLOW_HOME}}/data/olist/`
# 2. Execute esta DAG manualmente para gerar: `dados_consolidados.csv`
# 3. O objetivo Ã© demonstrar controle de orquestraÃ§Ã£o e tratamento de merges.
# =================================================================================

from __future__ import annotations
import os
import pendulum
import pandas as pd

from airflow.models.dag import DAG
from airflow.operators.python import PythonOperator


# === ğŸ”§ FunÃ§Ã£o principal de consolidaÃ§Ã£o de dados ===
def _consolidar_dados_olist():
    """
    LÃª mÃºltiplos arquivos CSV da Olist, executa joins, 
    e salva um dataset unificado.
    """
    base_path = '{{AIRFLOW_HOME}}/data/olist'
    print(f"ğŸ“ Lendo datasets a partir de: {base_path}")

    try:
        # ğŸ“¦ Leitura dos arquivos de dados
        clientes = pd.read_csv(f'{base_path}/olist_customers_dataset.csv')
        pedidos = pd.read_csv(f'{base_path}/olist_orders_dataset.csv')
        pagamentos = pd.read_csv(f'{base_path}/olist_order_payments_dataset.csv')
        itens = pd.read_csv(f'{base_path}/olist_order_items_dataset.csv')
        reviews = pd.read_csv(f'{base_path}/olist_order_reviews_dataset.csv')
        produtos = pd.read_csv(f'{base_path}/olist_products_dataset.csv')
    except FileNotFoundError as e:
        print(f"âŒ ERRO: Arquivo nÃ£o encontrado. Verifique se os dados estÃ£o em '{base_path}'.")
        raise e

    print("âš™ï¸ Iniciando merges...")

    # === ğŸ”— Etapa 1: Clientes e Pedidos ===
    dados_clientes = pd.merge(
        clientes[['customer_id', 'customer_city', 'customer_state']],
        pedidos[['order_id', 'order_status', 'customer_id']],
        on='customer_id', how='outer'
    )

    # === ğŸ”— Etapa 2: Pagamentos, Itens e Reviews ===
    dados_pedidos = pd.merge(
        pagamentos[['order_id', 'payment_type', 'payment_value']],
        itens[['order_id', 'product_id', 'price', 'freight_value']],
        on='order_id', how='outer'
    )
    dados_pedidos = pd.merge(
        dados_pedidos,
        reviews[['order_id', 'review_score']],
        on='order_id', how='outer'
    )
    dados_pedidos = pd.merge(
        dados_pedidos,
        produtos[['product_id', 'product_category_name']],
        on='product_id', how='left'
    )

    # === ğŸ”— Etapa 3: UniÃ£o final com dados do cliente ===
    dados_consolidados = pd.merge(
        dados_pedidos,
        dados_clientes,
        on='order_id',
        how='outer',
        suffixes=('_pedido', '_cliente')
    )

    print(f"ğŸ“Š Total de registros consolidados: {len(dados_consolidados)}")

    # === ğŸ’¾ Salvando resultado final ===
    output_file = f'{base_path}/dados_consolidados.csv'
    dados_consolidados.to_csv(output_file, index=False)
    print(f"âœ… Arquivo final salvo em: {output_file}")


# === ğŸ“… DefiniÃ§Ã£o da DAG ===
with DAG(
    dag_id='dag_consolida_dados_olist_v1',
    start_date=pendulum.datetime(2025, 6, 10, tz="UTC"),
    schedule=None,  # ExecuÃ§Ã£o manual
    catchup=False,
    doc_md="""
### DAG de ConsolidaÃ§Ã£o Olist
Esta DAG realiza a uniÃ£o de mÃºltiplos datasets pÃºblicos da Olist,
gerando um Ãºnico arquivo `.csv` contendo os dados consolidados.
""",
    tags=['olist', 'etl', 'consolidacao']
) as dag:

    # === â–¶ï¸ Tarefa Ãºnica de consolidaÃ§Ã£o ===
    tarefa_consolidar_dados = PythonOperator(
        task_id='executar_consolidacao_olist',
        python_callable=_consolidar_dados_olist
    )
